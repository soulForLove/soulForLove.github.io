<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>林林总总</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://blog.linzhongtai.cn/"/>
  <updated>2020-04-05T02:08:41.000Z</updated>
  <id>http://blog.linzhongtai.cn/</id>
  
  <author>
    <name>Soul</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>8、聚合函数count</title>
    <link href="http://blog.linzhongtai.cn/2020/04/8%E3%80%81%E8%81%9A%E5%90%88%E5%87%BD%E6%95%B0count/"/>
    <id>http://blog.linzhongtai.cn/2020/04/8、聚合函数count/</id>
    <published>2020-04-05T03:05:05.000Z</published>
    <updated>2020-04-05T02:08:41.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h2 id="count-的实现方式"><a href="#count-的实现方式" class="headerlink" title="count(*)的实现方式"></a>count(*)的实现方式</h2><p>在不同的MySQL引擎中，count(*)有不同的实现方式</p><ol><li>MyISAM引擎把一个表的总行数存在了磁盘上，因此执行count(*)的时候会直接返回这个数，效率很高；</li><li>而InnoDB引擎就麻烦了，它执行count(*)的时候，需要把数据一行一行地从引擎里面读出来，然后累积计数。<blockquote><p>以上是没有过滤条件的count(*)，如果加了where 条件的话，MyISAM表也是不能返回得这么快的。</p></blockquote></li></ol><p><strong>为什么InnoDB不跟MyISAM一样，也把数字存起来呢？</strong></p><p>因为即使是在同一个时刻的多个查询，由</p><p>于多版本并发控制（MVCC）的原因，InnoDB表“应该返回多少行”也是不确定的</p><p>假设表t中现在有10000条记录，我们设计了三个用户并行的会话。</p><ul><li>会话A先启动事务并查询一次表的总行数；</li><li>会话B启动事务，插入一行后记录后，查询表的总行数</li><li>会话C先启动一个单独的语句，插入一行记录后，查询表的总行数。</li></ul><p>我们假设从上到下是按照时间顺序执行的，同一行语句是在同一时刻执行的，<br>在最后一个时刻，三个会话A、B、C会同时查询表t的总行数，但拿到的结果却不同。<br><img src="https://blog2019.oss-cn-shenzhen.aliyuncs.com/mysql-8-1.png" alt="会话A、B、C的执行流程"><br>这和InnoDB的事务设计有关系，可重复读是它默认的隔离级别，在代码上就是通过多版本并发控制，也就是MVCC来实现的。每一行记录都要判断自己是否对这个会话可见，因此对于count(*)请求来说，InnoDB只好把数据一行一行地读出依次判断，可见的行才能够用于计算“基于这个查询”的表的总行数。</p><p>不过MySQL在执行count(*)操作的时候还是做了优化：<br><strong>MySQL优化器会找到最小的那棵树来遍历。在保证逻辑正确的前提下，尽量减少扫描的数据量，是数据库系统设计的通用法则之一</strong></p><p>小结：</p><ul><li>MyISAM表虽然count(*)很快，但是不支持事务；</li><li>show table status命令虽然返回很快，但是不准确；</li><li>InnoDB表直接count(*)会遍历全表，虽然结果准确，但会导致性能问题。</li></ul><h2 id="用缓存系统保存计数"><a href="#用缓存系统保存计数" class="headerlink" title="用缓存系统保存计数"></a>用缓存系统保存计数</h2><p>对于更新很频繁的库来说，可能会第一时间想到，用缓存系统来支持。<br>用一个Redis服务来保存这个表的总行数。这个表每被插入一行Redis计数就加1，每被删除一行Redis计数就减1。这种方式下，读和更新操作都很快，但是这种方式存在问题：<strong>可能会丢失更新</strong></p><p>Redis的数据不能永久地留在内存里，所以会把这个值定期地持久化存储起来。但即使这样，仍然可能丢失更新。试想如果刚刚在数据表中插入了一行，Redis中保存的值也加了1，然后Redis异常重启了，重启后要从存储redis数据的地方把这个值读回来，而刚刚加1的这个计数操作却丢失了。</p><p>虽然这个场景可以解决：在Redis异常重启以后，到数据库里面单独执行一次count(*)获取真实的行数，再把这个值写回到Redis里。异常重启毕竟不是经常出现的情况，这一次全表扫描的成本，还是可以接受的。<br><strong>不过，将计数保存在缓存系统中的方式，还不只是丢失更新的问题。即使Redis正常工作，这个值还是逻辑上不精确的。</strong></p><p>如果要显示操作记录的总数，同时还要显示最近操作的100条记录。那么，这个页面的逻辑就需要先到Redis里面取出计数，再到数据表里面取数据记录。</p><p>我们是这么定义不精确的：</p><ul><li>一种是，查到的100行结果里面有最新插入记录，而Redis的计数里还没加1；</li><li>另一种是，查到的100行结果里没有最新插入的记录，而Redis的计数里已经加了1。</li></ul><p>这两种情况，都是逻辑不一致的。<br><img src="https://blog2019.oss-cn-shenzhen.aliyuncs.com/mysql-8-2.png" alt="image.png"><br><img src="https://blog2019.oss-cn-shenzhen.aliyuncs.com/mysql-8-3.png" alt="image.png"></p><p>在并发系统里面，无法精确控制不同线程的执行时刻的，因为存在图中的这种操作序列，所以，即使Redis正常工作，这个计数值还是逻辑上不精确的。</p><h2 id="在数据库保存计数"><a href="#在数据库保存计数" class="headerlink" title="在数据库保存计数"></a>在数据库保存计数</h2><p><strong>如果我们把这个计数直接放到数据库里单独的一张计数表C中，又会怎么样呢？</strong></p><p>首先，这解决了崩溃丢失的问题，InnoDB是支持崩溃恢复不丢数据的。<br>其次，由于InnoDB是支持事务的，所以从下图可以看出逻辑上是一致的<br><img src="https://blog2019.oss-cn-shenzhen.aliyuncs.com/mysql-8-4.png" alt="image.png"></p><h2 id="不同的count用法"><a href="#不同的count用法" class="headerlink" title="不同的count用法"></a>不同的count用法</h2><blockquote><p>count()是一个聚合函数，对于返回的结果集，一行行地判断，如果count函数的参数不是NULL，累计值就加1，否则不加。最后返回累计值。</p></blockquote><ul><li>count(*)、count(主键id)和count(1) 都表示返回满足条件的结果集的总行数；</li><li>count(字段），则表示返回满足条件的数据行里面，参数“字段”不为NULL的总个数。</li></ul><ol><li><p>对于count(主键id)来说：InnoDB引擎会遍历整张表，把每一行的id值都取出来，返回给server层。server层拿到id后，判断是不可能为空的，就按行累加。</p></li><li><p>对于count(1)来说，InnoDB引擎遍历整张表，但不取值。server层对于返回的每一行，放一个数字“1”进去，判断是不可能为空的，按行累加。</p></li></ol><blockquote><p>单看这两个用法的差别的话，你能对比出来，count(1)执行得要比count(主键id)快。因为从引擎返回id会涉及到解析数据行，以及拷贝字段值的操作。</p></blockquote><ol start="3"><li>对于count(字段)来说：</li></ol><ul><li>如果这个“字段”是定义为not null的话，一行行地从记录里面读出这个字段，判断不能为null，按行累加；</li><li>如果这个“字段”定义允许为null，那么执行的时候，判断到有可能是null，还要把值取出来再判断一下，不是null才累加。</li></ul><ol start="4"><li>count(*)是例外，并不会把全部字段取出来，而是专门做了优化，不取值。count(*)肯定不是null，按行累加。</li></ol><blockquote><p>按照效率排序的话，count(字段)&lt;count(主键id)&lt;count(1)≈count(*)，建议尽量使用count(*)。</p></blockquote>]]></content>
    
    <summary type="html">
    
      count(字段) &lt; count(主键id) &lt; count(1) ≈ count(*)
    
    </summary>
    
      <category term="Mysql" scheme="http://blog.linzhongtai.cn/categories/Mysql/"/>
    
    
      <category term="Mysql" scheme="http://blog.linzhongtai.cn/tags/Mysql/"/>
    
      <category term="MySql实战45讲笔记" scheme="http://blog.linzhongtai.cn/tags/MySql%E5%AE%9E%E6%88%9845%E8%AE%B2%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>7、事务隔离机制</title>
    <link href="http://blog.linzhongtai.cn/2020/03/7%E3%80%81%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E6%9C%BA%E5%88%B6/"/>
    <id>http://blog.linzhongtai.cn/2020/03/7、事务隔离机制/</id>
    <published>2020-03-28T03:04:44.000Z</published>
    <updated>2020-03-28T02:08:41.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><blockquote><p>RC（Read Committed，读提交）<br>RR（Repeatable Read，可重复读）</p></blockquote><ol><li><p>innodb支持RC和RR隔离级别实现是用的一致性视图(consistent read view)</p></li><li><p>事务在启动时会拍一个快照,这个快照是基于整个库的.<br>基于整个库的意思就是说一个事务内,整个库的修改对于该事务都是不可见的(对于快照读的情况)</p><blockquote><p>增删改数据（DML)，修改表结构的操作（DDL）</p></blockquote></li></ol><p>如果在事务内select t表,另外的事务执行了DDL t表,根据发生时间,要嘛锁住要嘛报错</p><ol start="3"><li><p>事务是如何实现的MVCC呢?<br>(1)每个事务都有一个事务ID,叫做transaction id(严格递增)<br>(2)事务在启动时,找到已提交的最大事务ID记为up_limit_id。<br>(3)事务在更新一条语句时,比如id=1改为了id=2.会把id=1和该行之前的row trx_id写到undo log里,<br>并且在数据页上把id的值改为2,并且把修改这条语句的transaction id记在该行行头<br>(4)再定一个规矩,一个事务要查看一条数据时,必须先用该事务的up_limit_id与该行的transaction id做比对,<br>如果up_limit_id&gt;=transaction id,那么可以看.如果up_limit_id<transaction id,则只能去undo="" log里去取。去undo="" log查找数据的时候,也需要做比对,必须up_limit_id="">transaction id,才返回数据</transaction></p></li><li><p>什么是当前读,由于当前读都是先读后写,只能读当前的值,所以为当前读.会更新事务内的up_limit_id为该事务的transaction id</p></li><li><p>为什么rr能实现可重复读而rc不能,分两种情况<br>(1)快照读的情况下,rr不能更新事务内的up_limit_id,<br>而rc每次会把up_limit_id更新为快照读之前最新已提交事务的transaction id,则rc不能可重复读<br>(2)当前读的情况下,rr是利用record lock+gap lock来实现的,而rc没有gap,所以rc不能可重复读 </p></li></ol>]]></content>
    
    <summary type="html">
    
      RC RR
    
    </summary>
    
      <category term="Mysql" scheme="http://blog.linzhongtai.cn/categories/Mysql/"/>
    
    
      <category term="Mysql" scheme="http://blog.linzhongtai.cn/tags/Mysql/"/>
    
      <category term="MySql实战45讲笔记" scheme="http://blog.linzhongtai.cn/tags/MySql%E5%AE%9E%E6%88%9845%E8%AE%B2%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>6、行锁：怎么减少行锁对性能的影响？</title>
    <link href="http://blog.linzhongtai.cn/2020/03/6%E3%80%81%E8%A1%8C%E9%94%81%EF%BC%9A%E6%80%8E%E4%B9%88%E5%87%8F%E5%B0%91%E8%A1%8C%E9%94%81%E5%AF%B9%E6%80%A7%E8%83%BD%E7%9A%84%E5%BD%B1%E5%93%8D%EF%BC%9F/"/>
    <id>http://blog.linzhongtai.cn/2020/03/6、行锁：怎么减少行锁对性能的影响？/</id>
    <published>2020-03-14T02:08:41.000Z</published>
    <updated>2020-03-14T02:08:41.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h2 id="两阶段锁"><a href="#两阶段锁" class="headerlink" title="两阶段锁"></a>两阶段锁</h2><blockquote><p>在 InnoDB 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放， 而是要等到事务结束时才释放。</p></blockquote><p><img src="https://blog2019.oss-cn-shenzhen.aliyuncs.com/mysql-6-1.png" alt="image.png"></p><ul><li>建议：如果事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。</li></ul><h2 id="死锁"><a href="#死锁" class="headerlink" title="死锁"></a>死锁</h2><p>当并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致这几个线程都进入无限等待的状态。</p><p><img src="https://blog2019.oss-cn-shenzhen.aliyuncs.com/mysql-6-2.png" alt="image.png"></p><ul><li><strong>解决方案：</strong></li></ul><ol><li>通过参数 innodb_lock_wait_timeout 根据实际业务场景来设置超时时间，InnoDB引擎默认值是50s。</li><li>发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数 innodb_deadlock_detect 设置为 on，表示开启这个逻辑（默认是开启状态）。</li></ol><p>在InnoDB中，innodb_lock_wait_timeout的默认值是50s，意味着如果采用第一个策略，当出现死锁以后，第一个被锁住的线程要过50s才会超时退出，然后其他线程才有可能继续执行。对于在线服务来说，这个等待时间往往是无法接受的。</p><p>主动死锁检测在发生死锁的时候，是能够快速发现并进行处理的，但是它也是有额外负担的。（<strong>死锁检测要耗费大量的CPU资源</strong>）<br>比如所有事务都要更新同一行的场景呢？<br>每个新来的被堵住的线程，都要判断会不会由于自己的加入导致了死锁，这是一个时间复杂度是O(n)的操作。假设有1000个并发线程要同时更新同一行，那么死锁检测操作就是100万这个量级的。虽然最终检测的结果是没有死锁，但是这期间要消耗大量的CPU资源。因此，你就会看到CPU利用率很高，但是每秒却执行不了几个事务。</p><ul><li>如何解决热点行更新导致的性能问题？</li></ul><ol><li>如果能确保这个业务一定不会出现死锁，可以临时把死锁检测关闭掉。一般不建议采用</li><li>控制并发度，对应相同行的更新，在进入引擎之前排队。这样在InnoDB内部就不会有大量的死锁检测工作了。</li><li>将热更新的行数据拆分成逻辑上的多行来减少锁冲突，但是业务复杂度可能会大大提高。</li></ol><blockquote><p>innodb行级锁是通过锁索引记录实现的，如果更新的列没建索引是会锁住整个表的。 </p></blockquote>]]></content>
    
    <summary type="html">
    
      根据加锁范围：MySQL里面的锁可以分为：全局锁、表级锁、行级锁
    
    </summary>
    
      <category term="Mysql" scheme="http://blog.linzhongtai.cn/categories/Mysql/"/>
    
    
      <category term="Mysql" scheme="http://blog.linzhongtai.cn/tags/Mysql/"/>
    
      <category term="MySql实战45讲笔记" scheme="http://blog.linzhongtai.cn/tags/MySql%E5%AE%9E%E6%88%9845%E8%AE%B2%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>5、全局锁和表锁</title>
    <link href="http://blog.linzhongtai.cn/2020/03/5%E3%80%81%E5%85%A8%E5%B1%80%E9%94%81%E5%92%8C%E8%A1%A8%E9%94%81/"/>
    <id>http://blog.linzhongtai.cn/2020/03/5、全局锁和表锁/</id>
    <published>2020-03-07T08:45:47.000Z</published>
    <updated>2020-03-07T08:45:47.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h2 id="全局锁："><a href="#全局锁：" class="headerlink" title="全局锁："></a>全局锁：</h2><blockquote><p>对整个数据库实例加锁。</p></blockquote><ul><li>MySQL提供加全局读锁的方法：Flush tables with read lock(FTWRL)<br>这个命令可以使整个库处于只读状态。使用该命令之后，数据更新语句、数据定义语句和更新类事务的提交语句等操作都会被阻塞。</li><li>使用场景：全库逻辑备份。</li><li>风险：<ol><li>如果在主库备份，在备份期间不能更新，业务停摆</li><li>如果在从库备份，备份期间不能执行主库同步的binlog，导致主从延迟</li></ol></li></ul><p>官方自带的逻辑备份工具mysqldump，当mysqldump使用参数–single-transaction的时候，会启动一个事务，确保拿到一致性视图。而由于MVCC的支持，这个过程中数据是可以正常更新的。</p><p>一致性读是好，但是前提是引擎要支持这个隔离级别。比如，对于MyISAM这种不支持事务的引擎，如果备份过程中有更新，总是只能取到最新的数据，那么就破坏了备份的一致性。这时，我们就需要使用FTWRL命令了。<br>所以，<strong>single-transaction方法只适用于所有的表使用事务引擎的库</strong>。如果有的表使用了不支持事务的引擎，那么备份就只能通过FTWRL方法。这往往是DBA要求业务开发人员使用InnoDB替代MyISAM的原因之一。</p><ul><li><strong>如果要全库只读，为什么不使用set global readonly=true的方式？</strong></li></ul><ol><li>在有些系统中，readonly的值会被用来做其他逻辑，比如判断主备库。所以修改global变量的方式影响太大，不建议使用。</li><li>在异常处理机制上有差异。如果执行FTWRL命令之后由于客户端发生异常断开，那么MySQL会自动释放这个全局锁，整个库回到可以正常更新的状态。而将整个库设置为readonly之后，如果客户端发生异常，则数据库就会一直保持readonly状态，这样会导致整个库长时间处于不可写状态，风险较高。</li></ol><h2 id="表级锁"><a href="#表级锁" class="headerlink" title="表级锁"></a>表级锁</h2><blockquote><p>MySQL里面表级锁有两种，一种是表锁，一种是元数据所(meta data lock,MDL)</p></blockquote><p>表锁的语法是:lock tables … read/write，可以用unlock tables主动释放锁，也可以在客户端断开的时候自动释放。lock tables语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象。<br>对于InnoDB这种支持行锁的引擎，一般不使用lock tables命令来控制并发，毕竟锁住整个表的影响面还是太大。</p><p>在MySQL 5.5版本中引入了MDL，当对一个表做增删改查操作的时候，加MDL读锁；当要对表做结构变更操作的时候，加MDL写锁。<strong>MDL锁是系统默认会加的</strong></p><ul><li>读锁之间不互斥，因此你可以有多个线程同时对一张表增删改查。</li><li>读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。因此，如果有两个线程要同时给一个表加字段，其中一个要等另一个执行完才能开始执行。</li></ul><p><img src="https://blog2019.oss-cn-shenzhen.aliyuncs.com/mysql-5-1.png" alt="image.png"></p><p>我们可以看到session A先启动，这时候会对表t加一个MDL读锁。由于session B需要的也是MDL读锁，因此可以正常执行。<br>之后session C会被blocked，是因为session A的MDL读锁还没有释放，而session C需要MDL写锁，因此只能被阻塞。<br>如果只有session C自己被阻塞还没什么关系，但是之后所有要在表t上新申请MDL读锁的请求也会被session C阻塞。前面我们说了，所有对表的增删改查操作都需要先申请MDL读锁，就都被锁住，等于这个表现在完全不可读写了。</p><p>如果某个表上的查询语句频繁，而且客户端有重试机制，也就是说超时后会再起一个新session再请求的话，这个库的线程很快就会爆满。</p><p><strong>事务中的MDL锁，在语句执行开始时申请，但是语句结束后并不会马上释放，而会等到整个事务提交后再释放。所以在做表结构变更的时候，一定要小心不要导致锁住线上查询和更新。</strong></p><p><strong>那么，如何安全地给小表加字段？</strong></p><p>首先要解决长事务，事务不提交，就会一直占着MDL锁。在MySQL的information_schema 库的 innodb_trx 表中，可以查到当前执行中的事务。如果要做DDL变更的表刚好有长事务在执行，要考虑先暂停DDL，或者kill掉这个长事务。</p><p><strong>而如果要变更的表是一个热点表，虽然数据量不大，但是上面的请求很频繁，此时不得不加个字段，该怎么做呢？</strong></p><p>这时候kill可能未必管用，因为新的请求马上就来了。比较理想的机制是，在alter table语句里面设定等待时间，如果在这个指定的等待时间里面能够拿到MDL写锁最好，拿不到也不要阻塞后面的业务语句，先放弃。之后再通过重试命令重复这个过程。</p>]]></content>
    
    <summary type="html">
    
      根据加锁范围：MySQL里面的锁可以分为：全局锁、表级锁、行级锁
    
    </summary>
    
      <category term="Mysql" scheme="http://blog.linzhongtai.cn/categories/Mysql/"/>
    
    
      <category term="Mysql" scheme="http://blog.linzhongtai.cn/tags/Mysql/"/>
    
      <category term="MySql实战45讲笔记" scheme="http://blog.linzhongtai.cn/tags/MySql%E5%AE%9E%E6%88%9845%E8%AE%B2%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>4、索引</title>
    <link href="http://blog.linzhongtai.cn/2020/03/4%E3%80%81%E7%B4%A2%E5%BC%95/"/>
    <id>http://blog.linzhongtai.cn/2020/03/4、索引/</id>
    <published>2020-03-02T14:05:55.000Z</published>
    <updated>2020-03-02T14:05:55.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h2 id="索引的常见模型"><a href="#索引的常见模型" class="headerlink" title="索引的常见模型"></a>索引的常见模型</h2><h3 id="哈希表"><a href="#哈希表" class="headerlink" title="哈希表"></a>哈希表</h3><blockquote><p>适用于只有等值查询的场景</p></blockquote><p><img src="https://blog2019.oss-cn-shenzhen.aliyuncs.com/mysql-4-1.png" alt="哈希表示意图"><br>以键-值（key-value）存储数据的结构，只要输入待查找的值即key，就可以找到其对应的值即Value。哈希的思路很简单，把值放在数组里，用一个哈希函数把key换算成一个确定的位置，然后把value放在数组的这个位置。<br>多个key值经过哈希函数的换算，可能会出现同一个值的情况。此时会拉出一个链表（不是有序的，所以做区间查询的速度很慢）。</p><h3 id="有序数组"><a href="#有序数组" class="headerlink" title="有序数组"></a>有序数组</h3><blockquote><p>在等值查询和范围查询场景中的性能就都非常优秀，当更新的成本太高，所以只适用于静态存储引擎</p></blockquote><p><img src="https://blog2019.oss-cn-shenzhen.aliyuncs.com/mysql-4-2.png" alt="有序数组示意图"></p><h3 id="搜索树"><a href="#搜索树" class="headerlink" title="搜索树"></a>搜索树</h3><p><img src="https://blog2019.oss-cn-shenzhen.aliyuncs.com/mysql-4-3.png" alt="二叉搜索树示意图"></p><ul><li>每个节点的左儿子小于父节点，父节点又小于右儿子</li><li>查询时间复杂度O(log(N))，更新时间复杂度O(log(N))</li><li>数据库存储大多不适用二叉树，因为树高过高，会适用N叉树</li></ul><p>在MySQL中，索引是在存储引擎层实现的，所以并没有统一的索引标准，即不同存储引擎的索引的工作方式并不一样。而即使多个存储引擎支持同一种类型的索引，其底层的实现也可能不同。由于InnoDB存储引擎在MySQL数据库中使用最为广泛，所以下面以InnoDB为例，分析一下其中的索引模型。</p><h2 id="InnoDB-的索引模型"><a href="#InnoDB-的索引模型" class="headerlink" title="InnoDB 的索引模型"></a>InnoDB 的索引模型</h2><blockquote><p>在InnoDB中，表都是根据主键顺序以索引的形式存放的，这种存储方式的表称为索引组织表。又因为前面我们提到的，InnoDB使用了B+树索引模型，所以数据都是存储在B+树中的。</p></blockquote><p>每一个索引在InnoDB里面对应一棵B+树。</p><p>假设，我们有一个主键列为ID的表，表中有字段k，并且在k上有索引。<br>这个表的建表语句是：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; create table T(</span><br><span class="line">id int primary key, </span><br><span class="line">k int not null, </span><br><span class="line">name varchar(16),</span><br><span class="line">index (k))engine=InnoDB;</span><br></pre></td></tr></table></figure></p><p>表中R1~R5的(ID,k)值分别为(100,1)、(200,2)、(300,3)、(500,5)和(600,6)，两棵树的示例示意图如下。<br><img src="https://blog2019.oss-cn-shenzhen.aliyuncs.com/mysql-4-4.png" alt="InnoDB的索引组织结构"></p><p>从图中不难看出，根据叶子节点的内容，索引类型分为主键索引和非主键索引。<br>主键索引的叶子节点存的是整行数据。在InnoDB里，主键索引也被称为聚簇索引（clustered index）。<br>非主键索引的叶子节点内容是主键的值。在InnoDB里，非主键索引也被称为二级索引（secondary index）。<br>根据上面的索引结构说明，我们来讨论一个问题：<strong>基于主键索引和普通索引的查询有什么区别？</strong></p><ul><li>如果语句是select * from T where ID=500，即主键查询方式，则只需要搜索ID这棵B+树；</li><li>如果语句是select * from T where k=5，即普通索引查询方式，则需要先搜索k索引树，得到ID的值为500，再到ID索引树搜索一次。这个过程称为回表。</li></ul><p>也就是说，基于非主键索引的查询需要多扫描一棵索引树。因此，我们在应用中应该尽量使用主键查询。</p><h2 id="索引维护"><a href="#索引维护" class="headerlink" title="索引维护"></a>索引维护</h2><p>B+树为了维护索引有序性，在插入新值的时候需要做必要的维护。以上面这个图为例，如果插入新的行ID值为700，则只需要在R5的记录后面插入一个新记录。如果新插入的ID值为400，就相对麻烦了，需要逻辑上挪动后面的数据，空出位置。<br>而更糟的情况是，如果R5所在的数据页已经满了，根据B+树的算法，这时候需要申请一个新的数据页，然后挪动部分数据过去。这个过程称为页分裂。在这种情况下，性能自然会受影响。<br>除了性能外，页分裂操作还影响数据页的利用率。原本放在一个页的数据，现在分到两个页中，整体空间利用率降低大约50%。<br>当然有分裂就有合并。当相邻两个页由于删除了数据，利用率很低之后，会将数据页做合并。合并的过程，可以认为是分裂过程的逆过程。</p><ul><li>你可能在一些建表规范里面见到过类似的描述，要求建表语句里一定要有自增主键。当然事无绝对，我们来分析一下哪些场景下应该使用自增主键，而哪些场景下不应该</li></ul><p>自增主键是指自增列上定义的主键，在建表语句中一般是这么定义的： NOT NULL PRIMARY KEY AUTO_INCREMENT。</p><p>插入新记录的时候可以不指定ID的值，系统会获取当前ID最大值加1作为下一条记录的ID值。<br>自增主键的插入数据模式，正符合了我们前面提到的递增插入的场景。每次插入一条新记录，都是追加操作，都不涉及到挪动其他记录，也不会触发叶子节点的分裂。<br><strong>而有业务逻辑的字段做主键，则往往不容易保证有序插入，这样写数据成本相对较高。</strong></p><ul><li>除了考虑性能外，我们还可以从存储空间的角度来看。假设你的表中确实有一个唯一字段，比如字符串类型的身份证号，那应该用身份证号做主键，还是用自增字段做主键呢？</li></ul><p>由于每个非主键索引的叶子节点上都是主键的值。如果用身份证号做主键，那么每个二级索引的叶子节点占用约20个字节，而如果用整型做主键，则只要4个字节，如果是长整型（bigint）则是8个字节。<br><strong>显然，主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小。</strong></p><p><strong>所以，从性能和存储空间方面考量，自增主键往往是更合理的选择。</strong></p><ul><li>有没有什么场景适合用业务字段直接做主键的呢？还是有的。比如，有些业务的场景需求是这样的：</li></ul><ol><li>只有一个索引；</li><li>该索引必须是唯一索引。</li></ol><p>这就是典型的KV场景，由于没有其他索引，所以也就不用考虑其他索引的叶子节点大小的问题。<br>这时候我们就要优先考虑上一段提到的“尽量使用主键查询”原则，直接将这个索引设置为主键，可以避免每次查询需要搜索两棵树。</p><h2 id="深入索引"><a href="#深入索引" class="headerlink" title="深入索引"></a>深入索引</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; create table T (</span><br><span class="line">ID int primary key,</span><br><span class="line">k int NOT NULL DEFAULT 0, </span><br><span class="line">s varchar(16) NOT NULL DEFAULT &apos;&apos;,</span><br><span class="line">index k(k))</span><br><span class="line">engine=InnoDB;</span><br><span class="line"></span><br><span class="line">insert into T values(100,1, &apos;aa&apos;),(200,2,&apos;bb&apos;),(300,3,&apos;cc&apos;),(500,5,&apos;ee&apos;),(600,6,&apos;ff&apos;),(700,7,&apos;gg&apos;);</span><br></pre></td></tr></table></figure><p><img src="https://blog2019.oss-cn-shenzhen.aliyuncs.com/mysql-4-5.png" alt="InnoDB的索引组织结构"></p><p>如果执行 select * from T where k between 3 and 5，需要执行几次树的搜索操作，会扫描多少行？</p><p>这条SQL查询语句的执行流程：</p><ol><li>在k索引树上找到k=3的记录，取得 ID = 300；</li><li>再到ID索引树查到ID=300对应的R3；</li><li>在k索引树取下一个值k=5，取得ID=500；</li><li>再回到ID索引树查到ID=500对应的R4；</li><li>在k索引树取下一个值k=6，不满足条件，循环结束。</li></ol><p>在这个过程中，回到主键索引树搜索的过程，我们称为回表。可以看到，这个查询过程读了k索引树的3条记录（步骤1、3和5），回表了两次（步骤2和4）。<br>在这个例子中，由于查询结果所需要的数据只在主键索引上有，所以不得不回表。那么，<strong>有没有可能经过索引优化，避免回表过程呢？</strong></p><h3 id="覆盖索引"><a href="#覆盖索引" class="headerlink" title="覆盖索引"></a>覆盖索引</h3><blockquote><p>由于覆盖索引可以减少树的搜索次数，显著提升查询性能，所以使用覆盖索引是一个常用的性能优化手段。</p></blockquote><p>如果执行的语句是select ID from T where k between 3 and 5，这时只需要查ID的值，而ID的值已经在k索引树上了，因此可以直接提供查询结果，不需要回表。也就是说，在这个查询里面，索引k已经“覆盖了”我们的查询需求，我们称为覆盖索引。</p><h3 id="最左前缀原则"><a href="#最左前缀原则" class="headerlink" title="最左前缀原则"></a>最左前缀原则</h3><blockquote><p>B+树这种索引结构，可以利用索引的“最左前缀”，来定位记录。</p></blockquote><p>为了直观地说明这个概念，我们用（name，age）这个联合索引来分析。<br><img src="https://blog2019.oss-cn-shenzhen.aliyuncs.com/mysql-4-6.png" alt="（name，age）索引示意图"></p><p><strong>在建立联合索引的时候，如何安排索引内的字段顺序?</strong></p><ul><li>索引的复用能力<br>因为可以支持最左前缀，所以当已经有了(a,b)这个联合索引后，一般就不需要单独在a上建立索引了<br><strong>如果通过调整顺序，可以少维护一个索引，那么这个顺序往往就是需要优先考虑采用的。</strong></li></ul><p>如果既有联合查询，又有基于a、b各自的查询呢？查询条件里面只有b的语句，是无法使用(a,b)这个联合索引的，这时候不得不维护另外一个索引，也就是说需要同时维护(a,b)、(b) 这两个索引。</p><p>这时候，要考虑的原则就是空间了。比如上面这个市民表的情况，name字段是比age字段大的 ，那么则建议创建一个（name,age)的联合索引和一个(age)的单字段索引。</p><h3 id="索引下推"><a href="#索引下推" class="headerlink" title="索引下推"></a>索引下推</h3><p>以市民表的联合索引（name, age）为例。如果现在有一个需求：检索出表中“名字第一个字是张，而且年龄是10岁的所有男孩”。那么，SQL语句是这么写的：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; select * from tuser where name like &apos;张%&apos; and age=10 and ismale=1;</span><br></pre></td></tr></table></figure></p><p>在MySQL 5.6之前，只能从ID3开始一个个回表。到主键索引上找出数据行，再对比字段值。<br><img src="https://blog2019.oss-cn-shenzhen.aliyuncs.com/mysql-4-8.png" alt="无索引下推执行流程"><br>而MySQL 5.6 引入的索引下推优化（index condition pushdown)， 可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。<br><img src="https://blog2019.oss-cn-shenzhen.aliyuncs.com/mysql-4-9.png" alt="索引下推执行流程"></p><p>以上这两个图里面，每一个虚线箭头表示回表一次。</p><p>无索引下推的图中，在(name,age)索引里面特意去掉了age的值，这个过程InnoDB并不会去看age的值，只是按顺序把“name第一个字是’张’”的记录一条条取出来回表。因此，需要回表4次。</p><p>而在索引下推的图中，InnoDB在(name,age)索引内部就判断了age是否等于10，对于不等于10的记录，直接判断并跳过。在这个例子中，只需要对ID4、ID5这两条记录回表取数据判断，就只需要回表2次。</p>]]></content>
    
    <summary type="html">
    
      索引的出现其实就是为了提高数据查询的效率，就像书的目录一样。
    
    </summary>
    
      <category term="Mysql" scheme="http://blog.linzhongtai.cn/categories/Mysql/"/>
    
    
      <category term="Mysql" scheme="http://blog.linzhongtai.cn/tags/Mysql/"/>
    
      <category term="MySql实战45讲笔记" scheme="http://blog.linzhongtai.cn/tags/MySql%E5%AE%9E%E6%88%9845%E8%AE%B2%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>3、事务隔离</title>
    <link href="http://blog.linzhongtai.cn/2020/03/3%E3%80%81%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB/"/>
    <id>http://blog.linzhongtai.cn/2020/03/3、事务隔离/</id>
    <published>2020-03-01T02:53:56.000Z</published>
    <updated>2020-03-01T02:53:56.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h2 id="隔离性与隔离级别"><a href="#隔离性与隔离级别" class="headerlink" title="隔离性与隔离级别"></a>隔离性与隔离级别</h2><ol><li>事务的特性：原子性、一致性、隔离性、持久性</li><li>多事务同时执行的时候，可能会出现的问题：脏读、不可重复读、幻读</li><li>事务隔离级别：读未提交、读提交、可重复读、串行化</li><li>不同事务隔离级别的区别：</li></ol><ul><li>读未提交：一个事务还未提交，它所做的变更就可以被别的事务看到</li><li>读提交：一个事务提交之后，它所做的变更才可以被别的事务看到</li><li>可重复读：一个事务执行过程中看到的数据是一致的。未提交的更改对其他事务是不可见的</li><li>串行化：对应一个记录会加读写锁，出现冲突的时候，后访问的事务必须等前一个事务执行完成才能继续执行</li></ul><ol start="5"><li>配置方法：启动参数transaction-isolation</li></ol><h2 id="事务隔离的实现"><a href="#事务隔离的实现" class="headerlink" title="事务隔离的实现"></a>事务隔离的实现</h2><p>每条记录在更新的时候都会同时记录一条回滚操作。同一条记录在系统中可以存在多个版本，这就是数据库的多版本并发控制（MVCC）。</p><ol><li>回滚日志什么时候删除？系统会判断当没有事务需要用到这些回滚日志的时候，回滚日志会被删除。</li><li>什么时候不需要了？当系统里么有比这个回滚日志更早的read-view的时候。</li><li>为什么尽量不要使用长事务。长事务意味着系统里面会存在很老的事务视图，在这个事务提交之前，回滚记录都要保留，这会导致大量占用存储空间。除此之外，长事务还占用锁资源，可能会拖垮库。</li></ol><h2 id="事务启动方式"><a href="#事务启动方式" class="headerlink" title="事务启动方式"></a>事务启动方式</h2><ol><li>显式启动事务语句，begin或者start transaction,提交commit，回滚rollback；</li><li>set autocommit=0，该命令会把这个线程的自动提交关掉。这样只要执行一个select语句，事务就启动，并不会自动提交，直到主动执行commit或rollback或断开连接。</li></ol><p>如果考虑多一次交互问题，可以使用commit work and chain语法。在autocommit为1的情况下，用begin显式启动的事务，如果执行commit则提交事务。如果执行 commit work and chain，则是提交事务并自动启动下一个事务，这样也省去了再次执行begin语句的开销。同时带来的好处是从程序开发的角度明确地知道每个语句是否处于事务中。<br>在information_schema库的innodb_trx这个表中查询长事务，比如下面这个语句，用于查找持续时间超过60s的事务。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">select * from information_schema.innodb_trx </span><br><span class="line">where TIME_TO_SEC(timediff(now(),trx_started))&gt;60</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      ACID（Atomicity、Consistency、Isolation、Durability，即原子性、一致性、隔离性、持久性）
    
    </summary>
    
      <category term="Mysql" scheme="http://blog.linzhongtai.cn/categories/Mysql/"/>
    
    
      <category term="Mysql" scheme="http://blog.linzhongtai.cn/tags/Mysql/"/>
    
      <category term="MySql实战45讲笔记" scheme="http://blog.linzhongtai.cn/tags/MySql%E5%AE%9E%E6%88%9845%E8%AE%B2%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>2、更新语句的执行以及日志</title>
    <link href="http://blog.linzhongtai.cn/2020/02/2%E3%80%81%E6%9B%B4%E6%96%B0%E8%AF%AD%E5%8F%A5%E7%9A%84%E6%89%A7%E8%A1%8C%E4%BB%A5%E5%8F%8A%E6%97%A5%E5%BF%97/"/>
    <id>http://blog.linzhongtai.cn/2020/02/2、更新语句的执行以及日志/</id>
    <published>2020-02-28T07:11:53.000Z</published>
    <updated>2020-02-28T07:11:53.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p><img src="https://blog2019.oss-cn-shenzhen.aliyuncs.com/mysql-2-1.png" alt="执行流程"><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; update T set c=c+1 where ID=2;</span><br></pre></td></tr></table></figure></p><p>执行语句前要先连接数据库，这是连接器的工作。</p><p>前面我们说过，在一个表上有更新的时候，跟这个表有关的查询缓存会失效，所以这条语句就会把表T上所有缓存结果都清空。这也就是我们一般不建议使用查询缓存的原因。</p><p>接下来，分析器会通过词法和语法解析知道这是一条更新语句。优化器决定要使用ID这个索引。然后，执行器负责具体执行，找到这一行，然后更新。</p><p>与查询流程不一样的是，更新流程还涉及两个重要的日志模块，它们正是我们今天要讨论的主角：<strong>redo log（重做日志）和 binlog（归档日志）</strong>。</p><h2 id="重要的日志模块：redo-log"><a href="#重要的日志模块：redo-log" class="headerlink" title="重要的日志模块：redo log"></a>重要的日志模块：redo log</h2><p>当有一条记录需要更新的时候，InnoDB引擎就会先把记录写到redo log（粉板）里面，并更新内存，这个时候更新就算完成了。同时，InnoDB引擎会在适当的时候，将这个操作记录更新到磁盘里面，而这个更新往往是在系统比较空闲的时候做。<br>InnoDB的redo log是固定大小的，比如可以配置为一组4个文件，每个文件的大小是1GB，那么总共就可以记录4GB的操作。从头开始写，写到末尾就又回到开头循环写，如下面这个图所示。<br><img src="https://blog2019.oss-cn-shenzhen.aliyuncs.com/mysql-2-2.png" alt="image.png"></p><p>write pos是当前记录的位置，一边写一边后移，写到第3号文件末尾后就回到0号文件开头。checkpoint是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件。</p><p>write pos和checkpoint之间的是还空着的部分，可以用来记录新的操作。如果write pos追上checkpoint，表示满了，这时候不能再执行新的更新，得停下来先擦掉一些记录，把checkpoint推进一下。</p><p>有了redo log，InnoDB就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为<strong>crash-safe</strong>。</p><h2 id="重要的日志模块：binlog"><a href="#重要的日志模块：binlog" class="headerlink" title="重要的日志模块：binlog"></a>重要的日志模块：binlog</h2><p>MySQL整体来看，其实就有两块：一块是Server层，它主要做的是MySQL功能层面的事情；还有一块是引擎层，负责存储相关的具体事宜。上面我们聊到的redo log是InnoDB引擎特有的日志，而Server层也有自己的日志，称为binlog（归档日志）。</p><p>最开始MySQL里并没有InnoDB引擎。MySQL自带的引擎是MyISAM，但是MyISAM没有crash-safe的能力，binlog日志只能用于归档。而InnoDB是另一个公司以插件形式引入MySQL的，既然只依靠binlog是没有crash-safe能力的，所以InnoDB使用另外一套日志系统——也就是redo log来实现crash-safe能力。</p><p>这两种日志有以下三点不同：</p><ol><li>redo log是InnoDB引擎特有的；binlog是MySQL的Server层实现的，所有引擎都可以使用。</li><li>redo log是物理日志，记录的是“在某个数据页上做了什么修改”；binlog是逻辑日志，记录的是这个语句的原始逻辑，比如“给ID=2这一行的c字段加1 ”。</li><li>redo log是循环写的，空间固定会用完；binlog是可以追加写入的。“追加写”是指binlog文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。</li></ol><p>有了对这两个日志的概念性理解，我们再来看执行器和InnoDB引擎在执行这个简单的update语句时的内部流程:</p><ol><li>执行器先找引擎取ID=2这一行。ID是主键，引擎直接用树搜索找到这一行。如果ID=2这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。</li><li>执行器拿到引擎给的行数据，把这个值加上1，比如原来是N，现在就是N+1，得到新的一行数据，再调用引擎接口写入这行新数据。</li><li>引擎将这行新数据更新到内存中，同时将这个更新操作记录到redo log里面，此时redo log处于prepare状态。然后告知执行器执行完成了，随时可以提交事务。</li><li>执行器生成这个操作的binlog，并把binlog写入磁盘。</li><li>执行器调用引擎的提交事务接口，引擎把刚刚写入的redo log改成提交（commit）状态，更新完成。<br><img src="https://blog2019.oss-cn-shenzhen.aliyuncs.com/mysql-2-3.png" alt="image.png"><br>图中浅色框表示是在InnoDB内部执行的，深色框表示是在执行器中执行的。</li></ol><p>最后三步看上去有点“绕”，将redo log的写入拆成了两个步骤：prepare和commit，这就是”<strong>两阶段提交</strong>“</p><h2 id="tip"><a href="#tip" class="headerlink" title="tip"></a>tip</h2><ul><li><p>redo log用于保证crash-safe能力。innodb_flush_log_at_trx_commit这个参数设置成1的时候，表示每次事务的redo log都直接持久化到磁盘。这个参数建议设置成1，这样可以保证MySQL异常重启之后数据不丢失。</p></li><li><p>sync_binlog这个参数设置成1的时候，表示每次事务的binlog都持久化到磁盘。这个参数建议设置成1，这样可以保证MySQL异常重启之后binlog不丢失。</p></li></ul>]]></content>
    
    <summary type="html">
    
      redo log/ binlog
    
    </summary>
    
      <category term="Mysql" scheme="http://blog.linzhongtai.cn/categories/Mysql/"/>
    
    
      <category term="Mysql" scheme="http://blog.linzhongtai.cn/tags/Mysql/"/>
    
      <category term="MySql实战45讲笔记" scheme="http://blog.linzhongtai.cn/tags/MySql%E5%AE%9E%E6%88%9845%E8%AE%B2%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>1、SQL查询语句的执行流程</title>
    <link href="http://blog.linzhongtai.cn/2020/02/1%E3%80%81SQL%E6%9F%A5%E8%AF%A2%E8%AF%AD%E5%8F%A5%E7%9A%84%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B/"/>
    <id>http://blog.linzhongtai.cn/2020/02/1、SQL查询语句的执行流程/</id>
    <published>2020-02-23T02:15:34.000Z</published>
    <updated>2020-02-23T02:15:34.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p><img src="https://blog2019.oss-cn-shenzhen.aliyuncs.com/mysql-1-1.png" alt="执行流程"></p><h2 id="连接器"><a href="#连接器" class="headerlink" title="连接器"></a>连接器</h2><blockquote><p>连接器负责跟客户端建立连接、获取权限、维持和管理连接</p></blockquote><p>连接命令<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql -h$ip -P$port -u$user -p</span><br></pre></td></tr></table></figure></p><p>连接完成后，如果你没有后续的动作，这个连接就处于空闲状态，你可以在show processlist命令中看到:<br><img src="https://blog2019.oss-cn-shenzhen.aliyuncs.com/mysql-1-2.png" alt="show processlist"><br>上图便是show processlist的结果，其中的Command列显示为“Sleep”的这一行，就表示现在系统里面有一个空闲连接。</p><p><strong>客户端如果太长时间没动静，连接器就会自动将它断开。这个时间是由参数wait_timeout控制的，默认值是8小时。</strong><br>如果在连接被断开之后，客户端再次发送请求的话，就会收到一个错误提醒： Lost connection to MySQL server during query。这时候如果你要继续，就需要重连，然后再执行请求了。<br>数据库里面，长连接是指连接成功后，如果客户端持续有请求，则一直使用同一个连接。短连接则是指每次执行完很少的几次查询就断开连接，下次查询再重新建立一个。<br>建立连<br>接的过程通常是比较复杂的，所以我建议你在使用中要尽量减少建立连接的动作，也就是尽量使用长连接。</p><p>但是全部使用长连接后，你可能会发现，有些时候MySQL占用内存涨得特别快，这是因为MySQL在执行过程中临时使用的内存是管理在连接对象里面的。这些资源会在连接断开的时候才释放。所以如果长连接累积下来，可能导致内存占用太大，被系统强行杀掉（OOM），从现象看就是MySQL异常重启了。</p><p>怎么解决这个问题呢？你可以考虑以下两种方案：</p><ul><li>定期断开长连接。使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连。</li><li>如果你用的是MySQL 5.7或更新版本，可以在每次执行一个比较大的操作后，通过执行 mysql_reset_connection来重新初始化连接资源。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。</li></ul><h2 id="查询缓存"><a href="#查询缓存" class="headerlink" title="查询缓存"></a>查询缓存</h2><p>MySQL拿到一个查询请求后，会先到查询缓存看看，之前是不是执行过这条语句。之前执行过的语句及其结果可能会以key-value对的形式，被直接缓存在内存中。key是查询的语句，value是查询的结果。如果你的查询能够直接在这个缓存中找到key，那么这个value就会被直接返回给客户端<br>如果语句不在查询缓存中，就会继续后面的执行阶段。执行完成后，执行结果会被存入查询缓存中。你可以看到，如果查询命中缓存，MySQL不需要执行后面的复杂操作，就可以直接返回结果，这个效率会很高。</p><p><strong>但是大多数情况下建议不要使用查询缓存，因为查询缓存往往弊大于利</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清空。</span><br></pre></td></tr></table></figure></p><p>对于更新压力大的数据库来说，查询缓存的命中率会非常低。除非你的业务就是有一张静态表，很长时间才会更新一次。比如，一个系统配置表，那这张表上的查询才适合使用查询缓存。</p><p>MySQL也提供了这种“按需使用”的方式。你可以将参数query_cache_type设置成DEMAND，这样对于默认的SQL语句都不使用查询缓存。而对于你确定要使用查询缓存的语句，可以用SQL_CACHE显式指定，像下面这个语句一样：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; select SQL_CACHE * from T where ID=10；</span><br></pre></td></tr></table></figure></p><p>不过，MySQL 8.0版本直接将查询缓存的整块功能删掉了，也就是说8.0开始彻底没有这个功能了。</p><h2 id="分析器"><a href="#分析器" class="headerlink" title="分析器"></a>分析器</h2><p>如果没有命中查询缓存，就要开始真正执行语句了。首先，MySQL需要知道你要做什么，因此需要对SQL语句做解析。</p><ol><li>词法分析</li></ol><p>输入的是由多个字符串和空格组成的一条SQL语句，MySQL需要识别出里面的字符串分别是什么，代表什么。<br>MySQL从输入的”select”这个关键字识别出来，这是一个查询语句。它也要把字符串“T”识别成“表名T”，把字符串“ID”识别成“列ID”。</p><ol start="2"><li>语法分析</li></ol><p>根据词法分析的结果，语法分析器会根据语法规则，判断你输入的这个SQL语句是否满足MySQL语法。</p><p>如果你的语句不对，就会收到“You have an error in your SQL syntax”的错误提醒，比如下面这个语句select少打了开头的字母“s”。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; elect * from t where ID=1;</span><br><span class="line"></span><br><span class="line">ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL </span><br><span class="line">server version for the right syntax to use near &apos;elect * from t where ID=1&apos; at line 1</span><br></pre></td></tr></table></figure></p><p>一般语法错误会提示第一个出现错误的位置，所以要关注的是紧接“use near”的内容。</p><h2 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h2><p>分析器之后，在开始执行之前，还要先经过优化器的处理</p><blockquote><p>优化器是在表里面有多个索引的时候，决定使用哪个索引；或者在一个语句有多表关联（join）的时候，决定各个表的连接顺序</p></blockquote><p><a href="">优化器如何选择索引</a></p><h2 id="执行器"><a href="#执行器" class="headerlink" title="执行器"></a>执行器</h2><p>开始执行的时候，要先判断一下你对这个表T有没有执行查询的权限，如果没有，就会返回没有权限的错误，如下所示(在工程实现上，如果命中查询缓存，会在查询缓存放回结果的时候，做权限验证。查询也会在优化器之前调用precheck验证权限)。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; select * from T where ID=10;</span><br><span class="line"></span><br><span class="line">ERROR 1142 (42000): SELECT command denied to user &apos;b&apos;@&apos;localhost&apos; for table &apos;T&apos;</span><br></pre></td></tr></table></figure></p><p>如果有权限，就打开表继续执行。打开表的时候，执行器就会根据表的引擎定义，去使用这个引擎提供的接口。</p><p>比如我们这个例子中的表T中，ID字段没有索引，那么执行器的执行流程是这样的：</p><ol><li>调用InnoDB引擎接口取这个表的第一行，判断ID值是不是10，如果不是则跳过，如果是则将这行存在结果集中；</li><li>调用引擎接口取“下一行”，重复相同的判断逻辑，直到取到这个表的最后一行。</li><li>执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集返回给客户端。</li></ol><p>至此，这个语句就执行完成了。</p><p>对于有索引的表，执行的逻辑也差不多。第一次调用的是“取满足条件的第一行”这个接口，之后循环取“满足条件的下一行”这个接口，这些接口都是引擎中已经定义好的。</p><p>你会在数据库的慢查询日志中看到一个rows_examined的字段，表示这个语句执行过程中扫描了多少行。这个值就是在执行器每次调用引擎获取数据行的时候累加的。</p><p>在有些场景下，执行器调用一次，在引擎内部则扫描了多行，因此<strong>引擎扫描行数跟rows_examined并不是完全相同的</strong></p>]]></content>
    
    <summary type="html">
    
      客户端-&gt;连接器-&gt;(查询缓存)-&gt;分析器-&gt;优化器-&gt;执行器
    
    </summary>
    
      <category term="Mysql" scheme="http://blog.linzhongtai.cn/categories/Mysql/"/>
    
    
      <category term="Mysql" scheme="http://blog.linzhongtai.cn/tags/Mysql/"/>
    
      <category term="MySql实战45讲笔记" scheme="http://blog.linzhongtai.cn/tags/MySql%E5%AE%9E%E6%88%9845%E8%AE%B2%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>Redis知识点整理</title>
    <link href="http://blog.linzhongtai.cn/2019/10/Redis%E7%9F%A5%E8%AF%86%E7%82%B9%E6%95%B4%E7%90%86/"/>
    <id>http://blog.linzhongtai.cn/2019/10/Redis知识点整理/</id>
    <published>2019-10-10T03:37:47.000Z</published>
    <updated>2020-04-05T08:32:12.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h2 id="优势"><a href="#优势" class="headerlink" title="优势"></a>优势</h2><ol><li>性能极高 – 内存操作，单线程，<strong>非阻塞I/O多路复用机制</strong></li><li>丰富的数据类型 – String、List、Hash、Set 、ZSet </li><li>原子 – Redis的所有操作都是原子性的，单个操作是原子性的。</li><li>丰富的特性 – 支持 publish/subscribe, key 过期等等特性。<h2 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h2><h3 id="string"><a href="#string" class="headerlink" title="string"></a>string</h3></li></ol><ul><li>数字或者字符串的操作</li><li>计数<h3 id="list"><a href="#list" class="headerlink" title="list"></a>list</h3></li><li>消息队列</li><li>lrange分页<h3 id="hash"><a href="#hash" class="headerlink" title="hash"></a>hash</h3></li><li>存储结构化信息：用户信息，登录信息，业务信息等等<h3 id="set"><a href="#set" class="headerlink" title="set"></a>set</h3></li><li>全局去重</li><li><p>利用交集并集差集等等进行计算结果</p><h3 id="zset"><a href="#zset" class="headerlink" title="zset"></a>zset</h3><blockquote><p>sorted set多了一个权重参数score,集合中的元素能够按score进行排列。</p></blockquote></li><li><p>排行榜应用，取TOP N操作</p></li><li>范围查找<h2 id="底层数据结构"><a href="#底层数据结构" class="headerlink" title="底层数据结构"></a><a href="https://blog.linzhongtai.cn/2018/06/Redis%E5%BA%95%E5%B1%82%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">底层数据结构</a></h2></li></ul><h3 id="简单动态字符串（simple-dynamic-string）SDS"><a href="#简单动态字符串（simple-dynamic-string）SDS" class="headerlink" title="简单动态字符串（simple dynamic string）SDS"></a>简单动态字符串（simple dynamic string）SDS</h3><blockquote><p>除了用来保存字符串以外，SDS还被用作缓冲区（buffer）AOF(<a href="http://blog.linzhongtai.cn/2018/05/Redis%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8/">持久化</a>)模块中的AOF缓冲区</p></blockquote><table><thead><tr><th>C 字符串</th><th>SDS</th></tr></thead><tbody><tr><td>获取字符串长度的复杂度为O（N)</td><td>获取字符串长度的复杂度为O(1)</td></tr><tr><td>API 是不安全的，可能会造成缓冲区溢出</td><td>API 是安全的，不会造成缓冲区溢出</td></tr><tr><td>修改字符串长度N次必然需要执行N次内存重分配</td><td>修改字符串长度N次最多执行N次内存重分配</td></tr><tr><td>只能保存文本数据</td><td>可以保存二进制数据和文本文数据</td></tr><tr><td>可以使用所有&lt;String.h&gt;库中的函数</td><td>可以使用一部分&lt;string.h&gt;库中的函数</td></tr></tbody></table><h3 id="链表"><a href="#链表" class="headerlink" title="链表"></a>链表</h3><blockquote><p>顺序存储对象信息，有用于缓存链表长度的属性，在插入删除对象功能中有良好性能，避免环的产生。<br><strong>列表键的底层实现之一就是链表</strong></p></blockquote><h3 id="字典"><a href="#字典" class="headerlink" title="字典"></a>字典</h3><blockquote><p>key-value 存储方式，通过hash值计算，判断key的存储，当容量过大，会通过rehash重新分配字典大小</p></blockquote><p>普通状态下的字典：<br><img src="https://blog2019.oss-cn-shenzhen.aliyuncs.com/redis-1.png" alt="image.png"></p><p><strong>解决哈希冲突</strong>：在插入一条新的数据时，会进行哈希值的计算，如果出现了hash值相同的情况，Redis 中采用了连地址法（separate chaining）来解决键冲突。每个哈希表节点都有一个next 指针，多个哈希表节点可以使用next 构成一个单向链表，被分配到同一个索引上的多个节点可以使用这个单向链表连接起来解决hash值冲突的问题。如下：<br><img src="https://blog2019.oss-cn-shenzhen.aliyuncs.com/redis-2.png" alt="image.png"><br><img src="https://blog2019.oss-cn-shenzhen.aliyuncs.com/redis-3.png" alt="image.png"></p><p>在插入后我们可以看到，dictEntry指向了k2，k2的next指向了k1，从而完成了一次插入操作（这里选择表头插入是因为哈希表节点中没有记录链表尾节点位置）</p><h4 id="Rehash"><a href="#Rehash" class="headerlink" title="Rehash"></a>Rehash</h4><p>随着对哈希表的不断操作，哈希表保存的键值对会逐渐的发生改变，为了让哈希表的负载因子维持在一个合理的范围之内，我们需要对哈希表的大小进行相应的扩展或者压缩，这时候，我们可以通过 rehash（重新散列）操作来完成</p><ol><li>目前的哈希表状态</li></ol><p>我们可以看到，哈希表中的每个节点都已经使用到了，这时候我们需要对哈希表进行拓展<br><img src="https://blog2019.oss-cn-shenzhen.aliyuncs.com/redis-4.png" alt="image.png"></p><ol start="2"><li>为哈希表分配空间</li></ol><p>哈希表空间分配规则：</p><p><strong>如果执行的是拓展操作，那么ht[1] 的大小为第一个大于等于ht[0].used*2的2的n次幂</strong></p><p><strong>如果执行的是收缩操作，那么ht[1] 的大小为第一个大于等于ht[0].used的2的n次幂</strong><br>因此这里我们为ht[1] 分配 空间为8，</p><p><img src="https://blog2019.oss-cn-shenzhen.aliyuncs.com/redis-5.png" alt="image.png"></p><ol start="3"><li>数据转移</li></ol><p>将ht[0]中的数据转移到ht[1]中，在转移的过程中，需要对哈希表节点的数据重新进行哈希值计算<br>数据转移后的结果：</p><p><img src="https://blog2019.oss-cn-shenzhen.aliyuncs.com/redis-6.png" alt="image.png"></p><ol start="4"><li>释放ht[0]</li></ol><p>将ht[0]释放，然后将ht[1]设置成ht[0]，最后为ht[1]分配一个空白哈希表：</p><p><img src="https://blog2019.oss-cn-shenzhen.aliyuncs.com/redis-7.png" alt="image.png"></p><ol start="5"><li>渐进式 rehash</li></ol><p>上面我们说到，在进行拓展或者压缩的时候，可以直接将所有的键值对rehash 到ht[1]中，这是因为数据量比较小。在实际开发过程中，这个rehash 操作并不是一次性、集中式完成的，而是分多次、渐进式地完成的。<br>渐进式rehash 的详细步骤：</p><p>1、为ht[1] 分配空间，让字典同时持有ht[0]和ht[1]两个哈希表<br>2、在几点钟维持一个索引计数器变量rehashidx，并将它的值设置为0，表示rehash 开始<br>3、在rehash 进行期间，每次对字典执行CRUD操作时（增加只针对ht[1]），程序除了执行指定的操作以外，<br>还会将ht[0]中的数据rehash 到ht[1]表中，并且将rehashidx加一<br>4、当ht[0]中所有数据转移到ht[1]中时，将rehashidx 设置成-1，表示rehash 结束</p><p>采用渐进式rehash 的好处在于它采取分而治之的方式，避免了集中式rehash 带来的庞大计算量。</p><h3 id="跳跃表"><a href="#跳跃表" class="headerlink" title="跳跃表"></a>跳跃表</h3><blockquote><p>Redis 只在两个地方用到了跳跃表，一个是实现有序集合键，另外一个是在集群节点中用作内部数据结构</p></blockquote><ol><li><p>跳跃表是有序集合的底层实现之一</p></li><li><p>主要有zskiplist 和zskiplistNode两个结构组成</p></li><li><p>每个跳跃表节点的层高都是1至32之间的随机数</p></li><li><p>在同一个跳跃表中，多个节点可以包含相同的分值，但每个节点的对象必须是唯一的</p></li><li><p>节点按照分值的大小从大到小排序，如果分值相同，则按成员对象大小排序</p></li></ol><h3 id="整数集合"><a href="#整数集合" class="headerlink" title="整数集合"></a>整数集合</h3><blockquote><p>整数集合是集合建的底层实现之一，当一个集合中只包含整数，且这个集合中的元素数量不多时，redis就会使用整数集合intset作为集合的底层实现</p></blockquote><ol><li><p>整数集合是集合建的底层实现之一</p></li><li><p>整数集合的底层实现为数组，这个数组以有序，无重复的范式保存集合元素，在有需要时，程序会根据新添加的元素类型改变这个数组的类型</p></li><li><p>升级操作为整数集合带来了操作上的灵活性，并且尽可能地节约了内存</p></li><li><p>整数集合只支持升级操作，不支持降级操作</p></li></ol><h3 id="压缩列表"><a href="#压缩列表" class="headerlink" title="压缩列表"></a>压缩列表</h3><blockquote><p>压缩列表（ziplist）是列表键和哈希键的底层实现之一。当一个列表键只包含少量列表项，并且每个列表项要么就是小整数值， 要么就是长度比较短的字符串，那么Redis就会使用压缩列表来做列表键的底层实现。</p></blockquote><ol><li><p>压缩列表是一种为了节约内存而开发的顺序型数据结构</p></li><li><p>压缩列表被用作列表键和哈希键的底层实现之一</p></li><li><p>压缩列表可以包含多个节点，每个节点可以保存一个字节数组或者整数值</p></li><li><p>添加新节点到压缩列表，可能会引发连锁更新操作，删除节点也可能会引发连锁更新</p></li></ol><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ol><li><p>Redis 数据库中的每个键值对的键和值都是一个对象。</p></li><li><p>Redis 共有字符串、列表、哈希、集合、有序集合五种类型的对象，每种类型的对象至少都有两种或以上的编码方式，不同的编码可以在不同的使用场景上优化对象的使用效率。</p></li><li><p>服务器在执行某些命令之前， 会先检查给定键的类型能否执行指定的命令， 而检查一个键的类型就是检查键的值对象的类型。</p></li><li><p>Redis 的对象系统带有引用计数实现的内存回收机制， 当一个对象不再被使用时， 该对象所占用的内存就会被自动释放。</p></li><li><p>Redis 会共享值为 0 到 9999 的字符串对象。</p></li><li><p>对象会记录自己的最后一次被访问的时间， 这个时间可以用于计算对象的空转时间</p></li></ol><h2 id="主从复制-哨兵"><a href="#主从复制-哨兵" class="headerlink" title="主从复制(哨兵)"></a>主从复制(哨兵)</h2><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>salve 启动成功连接到master后会发送一个sync命令，master接到命令后启动后台的存盘进程，同时收集所有接收到的用于修改数据集命令，进程执行完毕后，master将传送整个数据文件到slave，以完成一次完全同步，也就是<strong>全量复制</strong>，而slave服务在接收到数据后，存盘到内存中；master将新的修改命令依次传给slave，完成同步，此时为<strong>增量复制</strong></p><blockquote><p>只要重新连接master，一次完全同步（全量复制）将被自动执行</p></blockquote><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><p>由于所有的写操作都在master上，然后同步更新到slave上，所以从master同步到slave 机器有一定的延迟，当系统很繁忙的时候，延迟问题会更加严重，slave机器数量的增加也会使得这个问题更加严重</p><h3 id="哨兵模式"><a href="#哨兵模式" class="headerlink" title="哨兵模式"></a>哨兵模式</h3><ol><li><p>监控redis集群</p></li><li><p>如果发现某个redis节点运行出现状况，能够通知另外一个进程(例如它的客户端);</p></li><li><p>能够进行自动切换。当一个master节点不可用时，能够选举出master的多个slave(如果有超过一个slave的话)中的一个来作为新的master,其它的slave节点会将它所追随的master的地址改为被提升为master的slave的新地址。</p></li><li><p>哨兵为客户端提供服务发现，客户端链接哨兵，哨兵提供当前master的地址然后提供服务，如果出现切换，也就是master挂了，哨兵会提供客户端一个新地址。</p></li><li><p>支持集群</p></li></ol><h2 id="过期策略"><a href="#过期策略" class="headerlink" title="过期策略"></a>过期策略</h2><blockquote><p>redis采用的是定期删除+惰性删除策略</p></blockquote><ul><li><p>定期<br>定期删除，redis默认每个100ms检查，是否有过期的key,有过期key则删除。需要说明的是，redis不是每个100ms将所有的key检查一次，而是随机抽取进行检查(如果每隔100ms,全部key进行检查，redis岂不是卡死)。因此，如果只采用定期删除策略，会导致很多key到时间没有删除。</p></li><li><p>定时<br>用一个定时器来负责监视key,过期则自动删除。虽然内存及时释放，但是十分消耗CPU资源。在大并发请求下，CPU要将时间应用在处理请求，而不是删除key</p></li><li><p>懒惰<br>获取某个key的时候，redis会检查一下，这个key如果设置了过期时间那么是否过期了？如果过期了此时就会删除</p></li></ul><p>定期删除+惰性删除也仍然存在问题，如果定期删除没删除key，然后也没即时去请求key，也就是说惰性删除也没生效。这样下去redis的内存会越来越高。那么就应该采用内存淘汰机制。</p><h2 id="内存淘汰机制"><a href="#内存淘汰机制" class="headerlink" title="内存淘汰机制"></a>内存淘汰机制</h2><h3 id="常用淘汰算法"><a href="#常用淘汰算法" class="headerlink" title="常用淘汰算法"></a>常用淘汰算法</h3><ol><li>FIFO：First In First Out，先进先出。判断被存储的时间，离目前最远的数据优先被淘汰。</li><li>LRU：Least Recently Used，最近最少使用。判断最近被使用的时间，目前最远的数据优先被淘汰。</li><li>LFU：Least Frequently Used，最不经常使用。在一段时间内，数据被使用次数最少的，优先被淘汰</li></ol><h3 id="Redis提供的淘汰策略"><a href="#Redis提供的淘汰策略" class="headerlink" title="Redis提供的淘汰策略"></a>Redis提供的淘汰策略</h3><ul><li>noeviction：不进行置换，表示即使内存达到上限也不进行置换，所有能引起内存增加的命令都会返回error</li><li>allkeys-lru：优先删除掉最近最不经常使用的key，用以保存新数据</li><li>volatile-lru：只从设置失效（expire set）的key中选择最近最不经常使用的key进行删除，用以保存新数据</li><li>allkeys-random: 随机从all-keys中选择一些key进行删除，用以保存新数据</li><li>volatile-random: 只从设置失效（expire set）的key中，选择一些key进行删除，用以保存新数据</li><li>volatile-ttl: 只从设置失效（expire set）的key中，选出存活时间（TTL）最短的key进行删除，用以保存新数据<h2 id="持久化"><a href="#持久化" class="headerlink" title="持久化"></a><a href="https://blog.linzhongtai.cn/2018/05/Redis%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8/">持久化</a></h2></li></ul><h3 id="Redis中数据存储模式"><a href="#Redis中数据存储模式" class="headerlink" title="Redis中数据存储模式"></a>Redis中数据存储模式</h3><ol><li>cache-only：只做为“缓存”服务，不持久数据，数据在服务终止后将消失，此模式下也将不存在“数据恢复”的手段，是一种安全性低/效率高/容易扩展的方式；</li><li>persistence：为内存中的数据持久备份到磁盘文件，在服务重启后可以恢复，此模式下数据相对安全</li></ol><p>对于persistence持久化存储，Redis提供了两种持久化方法：</p><ol><li>Redis DataBase(简称RDB)</li><li>Append-only file (简称AOF)</li></ol><blockquote><p>如果同时开启两种持久化，会优先加载AOF进行恢复</p></blockquote><h3 id="RDB持久化：默认开启"><a href="#RDB持久化：默认开启" class="headerlink" title="RDB持久化：默认开启"></a>RDB持久化：默认开启</h3><blockquote><p>指定时间间隔进行快照存储</p></blockquote><p>优点：使用单独子进程来进行持久化，主进程不会进行任何IO操作，保证了redis的高性能<br>缺点：RDB是间隔一段时间进行持久化，如果持久化之间redis发生故障，会发生数据丢失。所以这种方式更适合数据要求不严谨的时候</p><ul><li>save：服务器进程进行快照存储（阻塞）</li><li>bgsave：进行异步快照存储（派生子进程处理，非阻塞）</li><li>异常恢复：redis-check-rdb</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">#dbfilename：持久化数据存储在本地的文件</span><br><span class="line">dbfilename dump.rdb</span><br><span class="line">#dir：持久化数据存储在本地的路径，如果是在/redis/redis-3.0.6/src下</span><br><span class="line">启动的redis-cli，则数据会存储在当前src目录下</span><br><span class="line">dir ./</span><br><span class="line">##snapshot触发的时机，save &lt;seconds&gt; &lt;changes&gt;  </span><br><span class="line">##如下为900秒后，至少有一个变更操作，才会snapshot  </span><br><span class="line">##对于此值的设置，</span><br><span class="line">需要谨慎，评估系统的变更操作密集程度  </span><br><span class="line">##可以通过save “”来关闭snapshot功能  </span><br><span class="line">#持久化(以快照的方式) 策略（默认）</span><br><span class="line">save 900 1       （15分钟变更一次）</span><br><span class="line">save 300 10     （5分钟变更10次）</span><br><span class="line">save 60 10000  （1分钟变更1万次）</span><br><span class="line">##当snapshot时出现错误无法继续时，是否阻塞客户端“变更操作”，</span><br><span class="line">“错误”可能因为磁盘已满/磁盘故障/OS级别异常等  </span><br><span class="line">stop-writes-on-bgsave-error yes  </span><br><span class="line">##是否启用rdb文件压缩，默认为“yes”，压缩往往意味着“额外的cpu消耗”，</span><br><span class="line">同时也意味这较小的文件尺寸以及较短的网络传输时间  </span><br><span class="line">rdbcompression yes</span><br></pre></td></tr></table></figure><h3 id="AOF持久化：默认不开启"><a href="#AOF持久化：默认不开启" class="headerlink" title="AOF持久化：默认不开启"></a>AOF持久化：默认不开启</h3><blockquote><p>以日志的形式来记录每个写操作，将redis执行过的所有写指令记录下来（读操作不记录），只许追加但不可以改写文件，redis启动之初会读取改文件进行重新构建数据</p></blockquote><ul><li>AOF通过保存所有修改数据库的写命令请求来记录服务器的数据库状态</li><li>AOF文件中的所有命令都会以Redis命令请求协议的格式保存</li></ul><p>优点：可以保持更高的数据完整性，如果设置追加file的时间是1s，如果redis发生故障，最多会丢失1s的数据（appendfsync—&gt;everysec）；且如果日志写入不完整支持redis-check-aof来进行日志修复；AOF文件没被rewrite之前（文件过大时会对命令进行合并重写），可以删除其中的某些命令（比如误操作的flushall）。</p><p>缺点：AOF文件比RDB文件大，且恢复速度慢，运行效率也比rdb慢。</p><h4 id="重写机制"><a href="#重写机制" class="headerlink" title="重写机制"></a>重写机制</h4><blockquote><p>当超过阈值，则启动内容压缩，只保留最小指令集，可使用命令：bgrewriteaof</p></blockquote><ol><li>定义</li></ol><p>AOF采用文件追加的方式持久化数据，所以文件会越来越大，为了避免这种情况发生，增加了重写机制</p><p>当AOF文件的大小超过了配置所设置的阙值时，Redis就会启动AOF文件压缩，只保留可以恢复数据的最小指令集，可以使用命令bgrewriteaof</p><ol start="2"><li>原理</li></ol><p>当AOF增长过大时，会fork出一条新的进程将文件重写(也是先写临时文件最后rename)，遍历新进程的内存数据，每条记录有一条set语句。<br>重写AOF文件并没有操作旧的AOF文件，而是将整个内存中的数据内容用命令的方式重写了一个新的aof文件（有点类似快照）</p><ol start="3"><li>触发机制</li></ol><p>Redis会记录上次重写时的AOF文件大小，默认配置时当AOF文件大小是上次rewrite后大小的一倍且文件大于64M时触发<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">auto-aof-rewrite-percentage 100  （一倍）</span><br><span class="line">auto-aof-rewrite-min-size 64mb</span><br></pre></td></tr></table></figure></p><h4 id="持久化策略"><a href="#持久化策略" class="headerlink" title="持久化策略"></a>持久化策略</h4><ul><li>appendfsync always (同步持久化，每次发生数据变更会被立即记录到磁盘，性能差但数据完整性比较好)</li><li>appendfsync everysec (异步操作，每秒记录，如果一秒钟内宕机，有数据丢失)</li><li>appendfsync no （将缓存回写的策略交给系统，linux 默认是30秒将缓冲区的数据回写硬盘的）</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">##此选项为aof功能的开关，默认为“no”，可以通过“yes”来开启aof功能  </span><br><span class="line">##只有在“yes”下，aof重写/文件同步等特性才会生效  </span><br><span class="line">appendonly yes  </span><br><span class="line"></span><br><span class="line">##指定aof文件名称  </span><br><span class="line">appendfilename appendonly.aof  </span><br><span class="line"></span><br><span class="line">##指定aof操作中文件同步策略，有三个合法值：always(记录立即同步，性能较差) everysec(每秒同步，官方推荐) no(将缓存回写的策略交给系统，linux 默认是30秒将缓冲区的数据回写硬盘的)，默认为everysec  </span><br><span class="line">appendfsync everysec  </span><br><span class="line">##在aof-rewrite期间，appendfsync是否暂缓文件同步，&quot;no&quot;表示“不暂缓”，“yes”表示“暂缓”，默认为“no”  </span><br><span class="line">no-appendfsync-on-rewrite no  </span><br><span class="line"></span><br><span class="line">##aof文件rewrite触发的最小文件尺寸(mb,gb),只有大于此aof文件大于此尺寸是才会触发rewrite，默认“64mb”  </span><br><span class="line">auto-aof-rewrite-min-size 64mb  </span><br><span class="line"></span><br><span class="line">##相对于“上一次”rewrite，本次rewrite触发时aof文件应该增长的百分比。  </span><br><span class="line">##每一次rewrite之后，redis都会记录下此时“新aof”文件的大小(例如A)，那么当aof文件增长到A*(1 + p)之后  </span><br><span class="line">##触发下一次rewrite，每一次aof记录的添加，都会检测当前aof文件的尺寸。  </span><br><span class="line">auto-aof-rewrite-percentage 100</span><br></pre></td></tr></table></figure><h3 id="比较"><a href="#比较" class="headerlink" title="比较"></a>比较</h3><ul><li>RDB与AOF同时开启 默认先加载AOF的配置文件</li><li>相同数据集，AOF文件要远大于RDB文件，恢复速度慢于RDB</li><li>AOF运行效率慢于RDB,但是同步策略效率好，不同步效率和RDB相同</li></ul><h2 id="衍生数据结构"><a href="#衍生数据结构" class="headerlink" title="衍生数据结构"></a>衍生数据结构</h2><h3 id="Bitmaps"><a href="#Bitmaps" class="headerlink" title="Bitmaps"></a>Bitmaps</h3><blockquote><p>一个以位为单位的数组，数组的每个单元只能存储0和1</p></blockquote><p>在我们平时开发过程中，会有一些boolean型数据需要存取，比如用户一年的签到记录，签了是 1，没签是 0，要记录 365 天。如果使用普通的 key/value，每个用户要记录 365 个，当用户上亿的时候，需要的存储空间是惊人的。</p><p>为了解决这个问题，Redis 提供了位图数据结构，这样每天的签到记录只占据一个位，365 天就是 365 个位，46 个字节 (一个稍长一点的字符串) 就可以完全容纳下，这就大大节约了存储空间。</p><h4 id="基本使用"><a href="#基本使用" class="headerlink" title="基本使用"></a>基本使用</h4><ol><li>setbit</li><li>getbit</li><li>bitcount：统计指定位置范围内 1 的个数</li><li>bitpos：查找指定范围内出现的第一个 0 或 1</li><li>bitfield：多位操作，子命令（get/set/incrby）<h3 id="HyperLogLog"><a href="#HyperLogLog" class="headerlink" title="HyperLogLog"></a>HyperLogLog</h3><blockquote><p>HyperLogLog 提供不精确的去重计数方案，标准误差是 0.81%</p></blockquote></li></ol><p>案例：统计网站每个网页每天的 UV 数据？<br>常用解决方案是利用set集合进行去重，但是访问量巨大的时候，需要的存储空间也会相当惊人。<br>但是如果精确度要求不那么高，使用HyperLogLog可以省下相当大的空间。<br>网传HyperLogLog与集合结构的占用空间对比图：</p><table><thead><tr><th>数据类型</th><th>1天</th><th>1月</th><th>1年</th></tr></thead><tbody><tr><td>set</td><td>80M</td><td>2.3G</td><td>28G</td></tr><tr><td>HyperLogLog</td><td>20K</td><td>600K</td><td>7M</td></tr></tbody></table><h4 id="基本使用-1"><a href="#基本使用-1" class="headerlink" title="基本使用"></a>基本使用</h4><ol><li>pfadd：增加计数</li><li>pfcount：获取计数</li><li>pfmerge：将多个 pf 计数值累加，比如两个页面的统计结果合并<h3 id="Geo"><a href="#Geo" class="headerlink" title="Geo"></a>Geo</h3><blockquote><p>用数据库来算附近的人</p></blockquote></li></ol><h4 id="GeoHash-算法"><a href="#GeoHash-算法" class="headerlink" title="GeoHash 算法"></a>GeoHash 算法</h4><p>GeoHash 算法将二维的经纬度数据映射到一维的整数，这样所有的元素都将在挂载到一条线上，距离靠近的二维坐标映射到一维后的点之间距离也会很接近。当我们想要计算「附近的人时」，首先将目标位置映射到这条线上，然后在这个一维的线上获取附近的点就行了。</p><p><img src="https://blog2019.oss-cn-shenzhen.aliyuncs.com/redis-8.png" alt="image.png"></p><p>将整个地球看成一个二维平面，然后划分成了一系列正方形的方格，设想一个正方形的蛋糕摆在你面前，利用二刀法进行切割，二刀下去均分分成四块小正方形，这四个小正方形可以分别标记为 00,01,10,11 四个二进制整数。编码之后，每个地图元素的坐标都将变成一个整数，通过这个整数可以还原出元素的坐标，整数越长，还原出来的坐标值的损失程度就越小。</p><p><img src="https://blog2019.oss-cn-shenzhen.aliyuncs.com/redis-9.gif" alt="image.gif"></p><p>GeoHash 算法会继续对这个整数做一次 base32 编码 (0-9,a-z 去掉 a,i,l,o 四个字母) 变成一个字符串。在 Redis 里面，经纬度使用 52 位的整数进行编码，放进了 zset 里面，zset 的 value 是元素的 key，score 是 GeoHash 的 52 位整数值。</p><p>通过 zset 的 score 排序就可以得到坐标附近的其它元素 (实际情况要复杂一些，不过这样理解足够了)，通过将 score 还原成坐标值就可以得到元素的原始坐标。</p><h4 id="基本使用-2"><a href="#基本使用-2" class="headerlink" title="基本使用"></a>基本使用</h4><ol><li>geoadd：携带集合名称以及多个经纬度名称三元组，注意这里可以加入多个三元组</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1:6379&gt; geoadd company 116.48105 39.996794 juejin</span><br><span class="line">(integer) 1</span><br><span class="line">127.0.0.1:6379&gt; geoadd company 116.514203 39.905409 ireader</span><br><span class="line">(integer) 1</span><br><span class="line">127.0.0.1:6379&gt; geoadd company 116.489033 40.007669 meituan</span><br><span class="line">(integer) 1</span><br><span class="line">127.0.0.1:6379&gt; geoadd company 116.562108 39.787602 jd 116.334255 40.027400 xiaomi</span><br><span class="line">(integer) 2</span><br></pre></td></tr></table></figure><ol start="2"><li>geodist：用来计算两个元素之间的距离，携带集合名称、2 个名称和距离单位</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1:6379&gt; geodist company juejin ireader km</span><br><span class="line">&quot;10.5501&quot;</span><br><span class="line">127.0.0.1:6379&gt; geodist company juejin meituan km</span><br><span class="line">&quot;1.3878&quot;</span><br><span class="line">127.0.0.1:6379&gt; geodist company juejin jd km</span><br><span class="line">&quot;24.2739&quot;</span><br><span class="line">127.0.0.1:6379&gt; geodist company juejin xiaomi km</span><br><span class="line">&quot;12.9606&quot;</span><br><span class="line">127.0.0.1:6379&gt; geodist company juejin juejin km</span><br><span class="line">&quot;0.0000&quot;</span><br></pre></td></tr></table></figure><ol start="3"><li>geopos： 获取集合中任意元素的经纬度坐标，可以一次获取多个</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1:6379&gt; geopos company juejin</span><br><span class="line">1) 1) &quot;116.48104995489120483&quot;</span><br><span class="line">   2) &quot;39.99679348858259686&quot;</span><br><span class="line">127.0.0.1:6379&gt; geopos company ireader</span><br><span class="line">1) 1) &quot;116.5142020583152771&quot;</span><br><span class="line">   2) &quot;39.90540918662494363&quot;</span><br><span class="line">127.0.0.1:6379&gt; geopos company juejin ireader</span><br><span class="line">1) 1) &quot;116.48104995489120483&quot;</span><br><span class="line">   2) &quot;39.99679348858259686&quot;</span><br><span class="line">2) 1) &quot;116.5142020583152771&quot;</span><br><span class="line">   2) &quot;39.90540918662494363&quot;</span><br></pre></td></tr></table></figure><ol start="4"><li>georadiusbymember：最为关键的指令，它可以用来查询指定元素附近的其它元素，参数非常复杂。</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"># 范围 20 公里以内最多 3 个元素按距离正排，它不会排除自身</span><br><span class="line">127.0.0.1:6379&gt; georadiusbymember company ireader 20 km count 3 asc</span><br><span class="line">1) &quot;ireader&quot;</span><br><span class="line">2) &quot;juejin&quot;</span><br><span class="line">3) &quot;meituan&quot;</span><br><span class="line"># 范围 20 公里以内最多 3 个元素按距离倒排</span><br><span class="line">127.0.0.1:6379&gt; georadiusbymember company ireader 20 km count 3 desc</span><br><span class="line">1) &quot;jd&quot;</span><br><span class="line">2) &quot;meituan&quot;</span><br><span class="line">3) &quot;juejin&quot;</span><br><span class="line"># 三个可选参数 withcoord withdist withhash 用来携带附加参数</span><br><span class="line"># withdist 很有用，它可以用来显示距离</span><br><span class="line">127.0.0.1:6379&gt; georadiusbymember company ireader 20 km withcoord withdist withhash count 3 asc</span><br><span class="line">1) 1) &quot;ireader&quot;</span><br><span class="line">   2) &quot;0.0000&quot;</span><br><span class="line">   3) (integer) 4069886008361398</span><br><span class="line">   4) 1) &quot;116.5142020583152771&quot;</span><br><span class="line">      2) &quot;39.90540918662494363&quot;</span><br><span class="line">2) 1) &quot;juejin&quot;</span><br><span class="line">   2) &quot;10.5501&quot;</span><br><span class="line">   3) (integer) 4069887154388167</span><br><span class="line">   4) 1) &quot;116.48104995489120483&quot;</span><br><span class="line">      2) &quot;39.99679348858259686&quot;</span><br><span class="line">3) 1) &quot;meituan&quot;</span><br><span class="line">   2) &quot;11.5748&quot;</span><br><span class="line">   3) (integer) 4069887179083478</span><br><span class="line">   4) 1) &quot;116.48903220891952515&quot;</span><br><span class="line">      2) &quot;40.00766997707732031&quot;</span><br></pre></td></tr></table></figure><p>如果使用 Redis 的 Geo 数据结构，它们将全部放在一个 zset 集合中。在 Redis 的集群环境中，集合可能会从一个节点迁移到另一个节点，如果单个 key 的数据过大，会对集群的迁移工作造成较大的影响，在集群环境中单个 key 对应的数据量不宜超过 1M，否则会导致集群迁移出现卡顿现象，影响线上服务的正常运行。所以建议 Geo 的数据使用单独的 Redis 实例部署，不使用集群环境</p><h2 id="集群"><a href="#集群" class="headerlink" title="集群"></a>集群</h2><h3 id="Codis（豌豆荚）"><a href="#Codis（豌豆荚）" class="headerlink" title="Codis（豌豆荚）"></a>Codis（豌豆荚）</h3><p>一整套缓存解决方案，包含高可用、数据分片、监控、动态扩态 etc。走的是 Apps-&gt;代理-&gt;redis cluster<br><img src="https://blog2019.oss-cn-shenzhen.aliyuncs.com/redis-10.png" alt="image.png"></p><h3 id="Cluster（官方）"><a href="#Cluster（官方）" class="headerlink" title="Cluster（官方）"></a>Cluster（官方）</h3><p><img src="https://blog2019.oss-cn-shenzhen.aliyuncs.com/redis-11.png" alt="image.png"><br>没有proxy, 数据分片之后, 可以访问集群中任意一个节点来确定要访问的数据在哪个节点, 是一种将存储和元数据管理放一起的设计, 实现较复杂, 出现问题不好排查, 在大规模集群+多地域情况下不适合, 因为会导致较高的网络通讯成本</p><h3 id="Twemproxy（Twitter）"><a href="#Twemproxy（Twitter）" class="headerlink" title="Twemproxy（Twitter）"></a>Twemproxy（Twitter）</h3><p><img src="https://blog2019.oss-cn-shenzhen.aliyuncs.com/redis-12.png" alt="image.png"></p><p><a href="https://techlog.cn/article/list/10183287" target="_blank" rel="noopener">基于 Twemproxy 与 Codis 的 redis 集群方案比较</a></p><h2 id="拓展"><a href="#拓展" class="headerlink" title="拓展"></a>拓展</h2><h3 id="Scan-VS-Keys"><a href="#Scan-VS-Keys" class="headerlink" title="Scan VS Keys"></a>Scan VS Keys</h3><blockquote><p>两个命令都是遍历Redis所有key或者查找指定模式的key</p></blockquote><h4 id="keys"><a href="#keys" class="headerlink" title="keys"></a>keys</h4><ol><li>没有offset、limit参数，不能限制查询个数</li><li>keys是遍历算法，复杂度O(n)，数据量大的时候会导致redis卡顿<h4 id="Scan"><a href="#Scan" class="headerlink" title="Scan"></a>Scan</h4></li><li>复杂度O(n)，但是scan是通过游标分步进行，不阻塞</li><li>提供limit，可控制返回结果数</li><li>同keys一样，提供模式匹配</li><li>服务器不需要为游标保存状态，唯一状态是scan返回客户端的游标整数</li><li>返回结果可能重复，需要客户端去重</li><li>如果遍历过程中有数据修改，改动后的数据不保证同步</li><li>单次返回结果是空的，不表示遍历结束，而要看返回的游标值是否为0</li></ol><h3 id="缓存事件"><a href="#缓存事件" class="headerlink" title="缓存事件"></a>缓存事件</h3><h4 id="缓存雪崩"><a href="#缓存雪崩" class="headerlink" title="缓存雪崩"></a>缓存雪崩</h4><blockquote><p>即缓存同一时间大面积的失效，这个时候又来了一波请求，结果请求都怼到数据库上，从而导致服务崩了</p></blockquote><p>解决方案:</p><ol><li><p>给缓存的失效时间，加上一个随机值，避免集体失效。</p></li><li><p>使用互斥锁，但是该方案吞吐量明显下降了。</p></li><li><p>双缓存。我们有两个缓存，缓存A和缓存B。缓存A的失效时间为20分钟，缓存B不设失效时间。自己做缓存预热操作。然后细分以下几个小点：</p></li></ol><p>一：从缓存A读数据库，有则直接返回</p><p>二：A没有数据，直接从B读数据，直接返回，并且异步启动一个更新线程。</p><p>三：更新线程同时更新缓存A和缓存B。</p><h4 id="缓存穿透"><a href="#缓存穿透" class="headerlink" title="缓存穿透"></a>缓存穿透</h4><blockquote><p>即故意去请求缓存中不存在的数据，导致所有的请求都怼到数据库上，从而数据库连接异常。</p></blockquote><p>解决方案:</p><ol><li><p>利用互斥锁，缓存失效的时候，先去获得锁，得到锁了，再去请求数据库。没得到锁，则休眠一段时间重试</p></li><li><p>缓存空对象，设置过期时间</p></li><li><p>提供一个能迅速判断请求是否有效的拦截机制（布隆过滤器）</p></li></ol><blockquote><p>布隆过滤器：内部维护一系列合法有效的key，可以迅速判断出请求所携带的Key是否存在，如果不存在则直接返回。</p></blockquote><h4 id="缓存击穿"><a href="#缓存击穿" class="headerlink" title="缓存击穿"></a>缓存击穿</h4><blockquote><p>大量的请求同时查询一个 key 时，此时这个key正好失效了，就会导致大量的请求都打到数据库上面去</p></blockquote><p>解决方案：</p><ol><li>使用互斥锁</li></ol><p>在根据key获得的value值为空时，先锁上，再从数据库加载，加载完毕，释放锁。若其他线程发现获取锁失败，则睡眠后重试。至于锁的类型，单机环境用并发包的Lock类型就行，集群环境则使用分布式锁( redis的setnx)</p><ol start="2"><li>布隆过滤器（ 当布隆过滤器说某个值存在时，这个值可能不存在；当它说不存在时，那就肯定不存在。）</li></ol><h3 id="Redis-VS-Memcache"><a href="#Redis-VS-Memcache" class="headerlink" title="Redis VS Memcache"></a>Redis VS Memcache</h3><table><thead><tr><th>\/</th><th>数据类型</th><th>单Key限制</th><th>持久化</th><th>内存利用率</th><th>性能</th></tr></thead><tbody><tr><td>Memcache</td><td>key-value</td><td>1MB</td><td>否</td><td>使用简单的key-value存储的话，Memcached的内存利用率更高</td><td>可使用多核，100k以上的数据中，Memcached性能要高于Redis</td></tr><tr><td>Redis</td><td>string/list/hash/set/sorted set</td><td>512MB</td><td>是(RDB/AOF)</td><td>采用hash结构来做key-value存储，由于其组合式的压缩，其内存利用率会高于Memcached</td><td>Redis使用单核，在存储小数据时比Memcached性能更高</td></tr></tbody></table>]]></content>
    
    <summary type="html">
    
      数据类型/底层数据结构/主从复制/过期策略/内存淘汰机制/集群/缓存事件
    
    </summary>
    
      <category term="redis" scheme="http://blog.linzhongtai.cn/categories/redis/"/>
    
    
      <category term="redis" scheme="http://blog.linzhongtai.cn/tags/redis/"/>
    
  </entry>
  
  <entry>
    <title>一致性Hash</title>
    <link href="http://blog.linzhongtai.cn/2019/10/%E4%B8%80%E8%87%B4%E6%80%A7Hash/"/>
    <id>http://blog.linzhongtai.cn/2019/10/一致性Hash/</id>
    <published>2019-10-07T08:50:18.000Z</published>
    <updated>2019-10-07T08:50:18.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p>转载出处：<a href="https://www.jianshu.com/p/e968c081f563" target="_blank" rel="noopener">深入浅出一致性Hash原理</a></p><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在解决分布式系统中负载均衡的问题时候可以使用Hash算法让固定的一部分请求落到同一台服务器上，这样每台服务器固定处理一部分请求（并维护这些请求的信息），起到负载均衡的作用。<br>但是普通的余数hash（hash(比如用户id)%服务器机器数）算法伸缩性很差，当新增或者下线服务器机器时候，用户id与服务器的映射关系会大量失效，一致性hash则利用hash环对其进行了改进。</p><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><blockquote><p>为了能直观的理解一致性hash原理，这里结合一个简单的例子来讲解，假设有4台服务器，地址为ip1,ip2,ip3,ip4</p></blockquote><ul><li>一致性hash是首先计算四个ip地址对应的hash值<br>hash(ip1),hash(ip2),hash(ip3),hash(ip3)，计算出来的hash值是0~最大正整数直接的一个值，这四个值在一致性hash环上呈现如下图：<br><img src="https://blog2019.oss-cn-shenzhen.aliyuncs.com/hash-1.png" alt="image.png"></li><li><p>hash环上顺时针从整数0开始，一直到最大正整数，我们根据四个ip计算的hash值肯定会落到这个hash环上的某一个点，至此我们把服务器的四个ip映射到了一致性hash环</p></li><li><p>当用户在客户端进行请求时候，首先根据hash(用户id)</p></li></ul><p>计算路由规则（hash值），然后看hash值落到了hash环的那个地方，根据hash值在hash环上的位置顺时针找距离最近的ip作为路由ip.<br><img src="https://blog2019.oss-cn-shenzhen.aliyuncs.com/hash-2.png" alt="image.png"><br>如上图可知user1,user2的请求会落到服务器ip2进行处理，User3的请求会落到服务器ip3进行处理，user4的请求会落到服务器ip4进行处理，user5,user6的请求会落到服务器ip1进行处理</p><blockquote><p>下面考虑当ip2的服务器挂了的时候会出现什么情况？</p></blockquote><p>当ip2的服务器挂了的时候，一致性hash环大致如下图：<br><img src="https://blog2019.oss-cn-shenzhen.aliyuncs.com/hash-3.png" alt="image.png"><br>根据顺时针规则可知user1,user2的请求会被服务器ip3进行处理，而其它用户的请求对应的处理服务器不变，也就是只有之前被ip2处理的一部分用户的映射关系被破坏了，并且其负责处理的请求被顺时针下一个节点委托处理</p><blockquote><p>下面考虑当新增机器的时候会出现什么情况？</p></blockquote><p>当新增一个ip5的服务器后，一致性hash环大致如下图：<br><img src="https://blog2019.oss-cn-shenzhen.aliyuncs.com/hash-4.png" alt="image.png"></p><p>根据顺时针规则可知之前user5的请求应该被ip5服务器处理，现在被新增的ip5服务器处理，其他用户的请求处理服务器不变，也就是新增的服务器顺时针最近的服务器的一部分请求会被新增的服务器所替代</p><h2 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h2><ol><li>单调性(Monotonicity)</li></ol><p>单调性是指如果已经有一些请求通过哈希分派到了相应的服务器进行处理，又有新的服务器加入到系统中时候，应保证原有的请求可以被映射到原有的或者新的服务器中去，而不会被映射到原来的其它服务器上去。 这个通过上面新增服务器ip5可以证明，新增ip5后，原来被ip1处理的user6现在还是被ip1处理，原来被ip1处理的user5现在被新增的ip5处理。</p><ol start="2"><li>分散性(Spread)</li></ol><p>分布式环境中，客户端请求时候可能不知道所有服务器的存在，可能只知道其中一部分服务器，在客户端看来他看到的部分服务器会形成一个完整的hash环。如果多个客户端都把部分服务器作为一个完整hash环，那么可能会导致，同一个用户的请求被路由到不同的服务器进行处理。这种情况显然是应该避免的，因为它不能保证同一个用户的请求落到同一个服务器。所谓分散性是指上述情况发生的严重程度。好的哈希算法应尽量避免尽量降低分散性。 一致性hash具有很低的分散性</p><ol start="3"><li>平衡性(Balance)</li></ol><p>平衡性也就是说负载均衡，是指客户端hash后的请求应该能够分散到不同的服务器上去。一致性hash可以做到每个服务器都进行处理请求，但是不能保证每个服务器处理的请求的数量大致相同，如下图<br><img src="https://blog2019.oss-cn-shenzhen.aliyuncs.com/hash-5.png" alt="image.png"></p><p><strong>服务器ip1,ip2,ip3经过hash后落到了一致性hash环上，从图中hash值分布可知ip1会负责处理大概80%的请求，而ip2和ip3则只会负责处理大概20%的请求，虽然三个机器都在处理请求，但是明显每个机器的负载不均衡，这样称为一致性hash的倾斜，虚拟节点的出现就是为了解决这个问题</strong></p><h2 id="虚拟节点"><a href="#虚拟节点" class="headerlink" title="虚拟节点"></a>虚拟节点</h2><p>当服务器节点比较少的时候会出现上节所说的一致性hash倾斜的问题，一个解决方法是多加机器，但是加机器是有成本的，那么就加虚拟节点，比如上面三个机器，每个机器引入1个虚拟节点后的一致性hash环的图如下：<br><img src="https://blog2019.oss-cn-shenzhen.aliyuncs.com/hash-6.png" alt="image.png"><br>其中ip1-1是ip1的虚拟节点，ip2-1是ip2的虚拟节点，ip3-1是ip3的虚拟节点。<br>可知当物理机器数目为M，虚拟节点为N的时候，实际hash环上节点个数为M*N。比如当客户端计算的hash值处于ip2和ip3或者处于ip2-1和ip3-1之间时候使用ip3服务器进行处理。</p><h2 id="均匀一致性hash"><a href="#均匀一致性hash" class="headerlink" title="均匀一致性hash"></a>均匀一致性hash</h2><p>上节我们使用虚拟节点后的图看起来比较均衡，但是如果生成虚拟节点的算法不够好很可能会得到下面的环：<img src="https://blog2019.oss-cn-shenzhen.aliyuncs.com/hash-7.png" alt="image.png"><br>可知每个服务节点引入1个虚拟节点后，情况相比没有引入前均衡性有所改善，但是并不均衡。<br>均衡的一致性hash应该是如下图：<br><img src="https://blog2019.oss-cn-shenzhen.aliyuncs.com/hash-8.png" alt="image.png"><br>均匀一致性hash的目标是如果服务器有N台，客户端的hash值有M个，那么每个服务器应该处理大概M/N个用户的，也就是每台服务器负载尽量均衡。</p>]]></content>
    
    <summary type="html">
    
      在解决分布式系统中负载均衡的问题时候可以使用Hash算法让固定的一部分请求落到同一台服务器上，这样每台服务器固定处理一部分请求（并维护这些请求的信息），起到负载均衡的作用。但是普通的余数hash（hash(比如用户id)%服务器机器数）算法伸缩性很差，当新增或者下线服务器机器时候，用户id与服务器的映射关系会大量失效，一致性hash则利用hash环对其进行了改进。
    
    </summary>
    
      <category term="杂烩" scheme="http://blog.linzhongtai.cn/categories/%E6%9D%82%E7%83%A9/"/>
    
    
      <category term="小知识" scheme="http://blog.linzhongtai.cn/tags/%E5%B0%8F%E7%9F%A5%E8%AF%86/"/>
    
      <category term="杂烩" scheme="http://blog.linzhongtai.cn/tags/%E6%9D%82%E7%83%A9/"/>
    
  </entry>
  
  <entry>
    <title>Spring @Component派生性</title>
    <link href="http://blog.linzhongtai.cn/2019/07/Spring%20@Component%E6%B4%BE%E7%94%9F%E6%80%A7/"/>
    <id>http://blog.linzhongtai.cn/2019/07/Spring @Component派生性/</id>
    <published>2019-07-06T11:16:30.000Z</published>
    <updated>2019-07-06T11:16:30.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><blockquote><p>Spring核心部分提供了几种内建模式的注解，如@Component、@Service、@Repository、@Controller、@RestController以及@Configrution，但是Spring Framework并不限于这些内建模式注解，也可以自定义模式注解，Spring模式注解可理解为@Component”派生”注解</p></blockquote><ul><li><strong>Spring Framework 2.x</strong>只支持单层的@Component派生</li><li><strong>Spring Framework 3.0.0.RELEASE 后</strong>开始支持@Component，但是仅仅支持两层@Component派生（AnnotationAttributesReadingVisitor仅支持了两个for循环）</li><li><strong>Spring Framework 4.0.0.RELEASE 后</strong>开始支持多层次的@Component派生（AnnotationAttributesReadingVisitor采用递归） </li></ul><p><strong>ClassPathBeanDefinitionScanner</strong>允许自定义类型过滤规则，因此，Dubbo的@Service在没有标注@Component的情况下，通过scanner.addIncludeFilter(newAnnotationTypeFilter(Service.class))的方式达到了识别@Service所标注类的目的，不过这种方式没有用到@Component的派生性</p>]]></content>
    
    <summary type="html">
    
      Spring模式注解可理解为@Component&quot;派生&quot;注解
    
    </summary>
    
      <category term="JAVA" scheme="http://blog.linzhongtai.cn/categories/JAVA/"/>
    
    
      <category term="JAVA" scheme="http://blog.linzhongtai.cn/tags/JAVA/"/>
    
  </entry>
  
  <entry>
    <title>SpringBoot如何解决项目启动时初始化资源</title>
    <link href="http://blog.linzhongtai.cn/2019/06/SpringBoot%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E9%A1%B9%E7%9B%AE%E5%90%AF%E5%8A%A8%E6%97%B6%E5%88%9D%E5%A7%8B%E5%8C%96%E8%B5%84%E6%BA%90/"/>
    <id>http://blog.linzhongtai.cn/2019/06/SpringBoot如何解决项目启动时初始化资源/</id>
    <published>2019-06-24T06:39:04.000Z</published>
    <updated>2019-06-24T06:39:04.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><blockquote><p>本文转发自《纯洁的微笑》，原文链接：<a href="http://www.ityouknow.com/springboot/2018/05/03/spring-boot-commandLineRunner.html" target="_blank" rel="noopener">Spring Boot 如何解决项目启动时初始化资源</a></p></blockquote><p>在我们实际工作中，总会遇到这样需求，在项目启动的时候需要做一些初始化的操作，比如初始化线程池，提前加载好加密证书等。今天就给大家介绍一个 Spring Boot 神器，专门帮助大家解决项目启动初始化资源操作。</p><p>这个神器就是 CommandLineRunner，CommandLineRunner 接口的 Component 会在所有 Spring Beans 都初始化之后，SpringApplication.run() 之前执行，非常适合在应用程序启动之初进行一些数据初始化的工作。</p><p>接下来我们就运用案例测试它如何使用，在测试之前在启动类加两行打印提示，方便我们识别 CommandLineRunner 的执行时机。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">@SpringBootApplication</span><br><span class="line">public class CommandLineRunnerApplication &#123;</span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">System.out.println(&quot;The service to start.&quot;);</span><br><span class="line">SpringApplication.run(CommandLineRunnerApplication.class, args);</span><br><span class="line">System.out.println(&quot;The service has started.&quot;);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>接下来我们直接创建一个类继承 CommandLineRunner ，并实现它的 run() 方法。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">@Component</span><br><span class="line">public class Runner implements CommandLineRunner &#123;</span><br><span class="line">    @Override</span><br><span class="line">    public void run(String... args) throws Exception &#123;</span><br><span class="line">        System.out.println(&quot;The Runner start to initialize ...&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>我们在 run() 方法中打印了一些参数来看出它的执行时机。完成之后启动项目进行测试：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">The service to start.</span><br><span class="line"></span><br><span class="line">  .   ____          _            __ _ _</span><br><span class="line"> /\\ / ___&apos;_ __ _ _(_)_ __  __ _ \ \ \ \</span><br><span class="line">( ( )\___ | &apos;_ | &apos;_| | &apos;_ \/ _` | \ \ \ \</span><br><span class="line"> \\/  ___)| |_)| | | | | || (_| |  ) ) ) )</span><br><span class="line">  &apos;  |____| .__|_| |_|_| |_\__, | / / / /</span><br><span class="line"> =========|_|==============|___/=/_/_/_/</span><br><span class="line"> :: Spring Boot ::        (v2.0.0.RELEASE)</span><br><span class="line">...</span><br><span class="line">2018-04-21 22:21:34.706  INFO 27016 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port(s): 8080 (http) with context path &apos;&apos;</span><br><span class="line">2018-04-21 22:21:34.710  INFO 27016 --- [           main] com.neo.CommandLineRunnerApplication     : Started CommandLineRunnerApplication in 3.796 seconds (JVM running for 5.128)</span><br><span class="line">The Runner start to initialize ...</span><br><span class="line">The service has started.</span><br></pre></td></tr></table></figure></p><p>根据控制台的打印信息我们可以看出 CommandLineRunner 中的方法会在 Spring Boot 容器加载之后执行，执行完成后项目启动完成。</p><p>如果我们在启动容器的时候需要初始化很多资源，并且初始化资源相互之间有序，那如何保证不同的 CommandLineRunner 的执行顺序呢？Spring Boot 也给出了解决方案。那就是使用 @Order 注解。</p><p>我们创建两个 CommandLineRunner 的实现类来进行测试：</p><p>第一个实现类：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">@Component</span><br><span class="line">@Order(1)</span><br><span class="line">public class OrderRunner1 implements CommandLineRunner &#123;</span><br><span class="line">    @Override</span><br><span class="line">    public void run(String... args) throws Exception &#123;</span><br><span class="line">        System.out.println(&quot;The OrderRunner1 start to initialize ...&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>第二个实现类：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">@Component</span><br><span class="line">@Order(2)</span><br><span class="line">public class OrderRunner2 implements CommandLineRunner &#123;</span><br><span class="line">    @Override</span><br><span class="line">    public void run(String... args) throws Exception &#123;</span><br><span class="line">        System.out.println(&quot;The OrderRunner2 start to initialize ...&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>添加完成之后重新启动，观察执行顺序：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">The service to start.</span><br><span class="line"></span><br><span class="line">  .   ____          _            __ _ _</span><br><span class="line"> /\\ / ___&apos;_ __ _ _(_)_ __  __ _ \ \ \ \</span><br><span class="line">( ( )\___ | &apos;_ | &apos;_| | &apos;_ \/ _` | \ \ \ \</span><br><span class="line"> \\/  ___)| |_)| | | | | || (_| |  ) ) ) )</span><br><span class="line">  &apos;  |____| .__|_| |_|_| |_\__, | / / / /</span><br><span class="line"> =========|_|==============|___/=/_/_/_/</span><br><span class="line"> :: Spring Boot ::        (v2.0.0.RELEASE)</span><br><span class="line">...</span><br><span class="line">2018-04-21 22:21:34.706  INFO 27016 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port(s): 8080 (http) with context path &apos;&apos;</span><br><span class="line">2018-04-21 22:21:34.710  INFO 27016 --- [           main] com.neo.CommandLineRunnerApplication     : Started CommandLineRunnerApplication in 3.796 seconds (JVM running for 5.128)</span><br><span class="line">The OrderRunner1 start to initialize ...</span><br><span class="line">The OrderRunner2 start to initialize ...</span><br><span class="line">The Runner start to initialize ...</span><br><span class="line">The service has started.</span><br></pre></td></tr></table></figure></p><p>通过控制台的输出我们发现，添加 @Order 注解的实现类最先执行，并且@Order()里面的值越小启动越早。</p>]]></content>
    
    <summary type="html">
    
      CommandLineRunner
    
    </summary>
    
      <category term="SpringBoot" scheme="http://blog.linzhongtai.cn/categories/SpringBoot/"/>
    
    
      <category term="SpringBoot" scheme="http://blog.linzhongtai.cn/tags/SpringBoot/"/>
    
  </entry>
  
  <entry>
    <title>Linux常用文件搜索命令</title>
    <link href="http://blog.linzhongtai.cn/2019/06/Linux%E5%B8%B8%E7%94%A8%E6%96%87%E4%BB%B6%E6%90%9C%E7%B4%A2%E5%91%BD%E4%BB%A4/"/>
    <id>http://blog.linzhongtai.cn/2019/06/Linux常用文件搜索命令/</id>
    <published>2019-06-13T02:59:46.000Z</published>
    <updated>2019-06-13T02:59:46.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h2 id="最强大的搜索命令：find"><a href="#最强大的搜索命令：find" class="headerlink" title="最强大的搜索命令：find"></a>最强大的搜索命令：find</h2><blockquote><p>首先进行一点说明，find命令是我们在Linux系统中用来进行文件搜索用的最多的命令，功能特别强大。但是我们要说的是尽量少用find命令去执行搜索任务，就算要搜索我们也应该尽量的缩小范围，也不要在服务器使用高峰期进行文件搜索，因为搜索也是很占系统资源的。这就需要我们在进行Linux文件整理的时候，尽量规范化，什么文件放在什么目录下都要有比较好的约定。</p></blockquote><ol><li>命令名称：find</li><li>命令所在路径：/bin/find</li><li>执行权限：所有用户</li><li>功能描述：进行各种花式文件搜索</li><li>语法：find【搜索范围】【匹配条件】</li></ol><p><strong>Linux搜索和windows是有明显区别的，Linux严格区分文件大小写。</strong></p><h3 id="文件或目录名称"><a href="#文件或目录名称" class="headerlink" title="文件或目录名称"></a>文件或目录名称</h3><p>find 【搜索目录】【-name或者-iname】【搜索字符】：-name和-iname的区别一个区分大小写，一个不区分大小写</p><ol><li>find /etc -name init   (精准搜索，名字必须为 init 才能搜索的到)</li><li>find /etc -iname init   (精准搜索，名字必须为 init或者有字母大写也能搜索的到)</li><li>find /etc -name *init  (模糊搜索，以 init 结尾的文件或目录名) </li><li>find /etc -name init??? (模糊搜索，？ 表示单个字符，即搜索到 init___)</li></ol><h3 id="文件大小"><a href="#文件大小" class="headerlink" title="文件大小"></a>文件大小</h3><p>比如：在根目录下查找大于 100M 的文件</p><blockquote><p>find / -size +204800</p></blockquote><p><strong>+n 表示大于，-n 表示小于，n 表示等于</strong><br>1 数据块 == 512 字节 ==0.5KB，也就是1KB等于2数据块<br>100MB == 102400KB==204800数据块</p><h3 id="所有者和所属组"><a href="#所有者和所属组" class="headerlink" title="所有者和所属组"></a>所有者和所属组</h3><ol><li><p>在home目录下查询所属组为 root 的文件</p><blockquote><p>find /home -group root</p></blockquote></li><li><p>在home目录下查询所有者为 root 的文件</p><blockquote><p>find /home -user root</p></blockquote></li></ol><h3 id="时间属性"><a href="#时间属性" class="headerlink" title="时间属性"></a>时间属性</h3><p>find 【路径】【选项】【时间】<br>选项有下面三种：</p><ul><li>-amin  访问时间</li><li>-cmin   文件属性被更改</li><li>-mmin  文件内容被修改</li></ul><p>时间：<strong>+n,-n,n分别表示超过n分钟，n分钟以内和n分钟</strong><br>范例：在 /etc 目录下查找5 分钟内被修改过属性的文件和目录</p><blockquote><p>find /etc -cmin -5</p></blockquote><h3 id="文件类型或i节点"><a href="#文件类型或i节点" class="headerlink" title="文件类型或i节点"></a>文件类型或i节点</h3><p><strong>-type 根据文件类型查找：f表示文件，d表示目录，l表示软链接</strong></p><p>范例：查找 /home 目录下文件类型是目录的   find /home -type l</p><p><strong>-inum 根据i节点查找</strong></p><p>范例：查找 /tmp 目录下i节点为400342的文件或目录  find /tmp -inum 400342</p><h3 id="组合条件"><a href="#组合条件" class="headerlink" title="组合条件"></a>组合条件</h3><ol><li>-a  表示两个条件同时满足（and）</li><li>-o  表示两个条件满足任意一个即可（or）</li></ol><p>范例：查找/etc目录下大于80MB同时小于100MB的文件</p><blockquote><p>find /etc -size +163840 -a -size -204800</p></blockquote><h2 id="在文件资料库中查找文件命令：locate"><a href="#在文件资料库中查找文件命令：locate" class="headerlink" title="在文件资料库中查找文件命令：locate"></a>在文件资料库中查找文件命令：locate</h2><ol><li>命令名称：locate</li><li>命令所在路径：/usr/bin/locate</li><li>执行权限：所有用户</li><li>功能描述：在文件资料库中查找文件</li><li>语法：locate【文件名】<br>-i  不区分大小写</li></ol><blockquote><p><strong>注意</strong>：这里和 find 命令是有区别的，find是全盘检索，而locate 是在文件资料库中进行搜索。所以locate命令的执行要比find命令执行速度快很多。但是这里有个问题，文件资料库是需要不断更新的。我们新创建的文件如果不更新 文件资料库，使用 locate 是查找不到的。</p></blockquote><blockquote><p>updatedb  手动更新资料库，但是对于/tmp目录下的新建文件，是更新不到文件资料库的，因为/tmp目录不属于文件资料库的收录范围。</p></blockquote><h2 id="搜索命令所在的目录及别名信息：which"><a href="#搜索命令所在的目录及别名信息：which" class="headerlink" title="搜索命令所在的目录及别名信息：which"></a>搜索命令所在的目录及别名信息：which</h2><ol><li>命令名称：which</li><li>命令所在路径：/usr/bin/which</li><li>执行权限：所有用户</li><li>功能描述：搜索命令所在的目录及别名信息</li><li>语法：which【命令】</li></ol><p>范例：查询 ls 命令所在目录以及别名信息</p><blockquote><p>which ls</p></blockquote><h2 id="搜索命令所在的目录及帮助文档路径：whereis"><a href="#搜索命令所在的目录及帮助文档路径：whereis" class="headerlink" title="搜索命令所在的目录及帮助文档路径：whereis"></a>搜索命令所在的目录及帮助文档路径：whereis</h2><ol><li>命令名称：whereis</li><li>命令所在路径：/usr/bin/whereis</li><li>执行权限：所有用户</li><li>功能描述：搜索命令所在的目录及帮助文档路径</li><li>语法：whereis【命令】</li></ol><p>范例：查询 ls 命令所在目录以及帮助文档路径</p><blockquote><p>whereis ls</p></blockquote><h2 id="在文件中搜寻字符串匹配的行并输出：grep"><a href="#在文件中搜寻字符串匹配的行并输出：grep" class="headerlink" title="在文件中搜寻字符串匹配的行并输出：grep"></a>在文件中搜寻字符串匹配的行并输出：grep</h2><ol><li>命令名称：grep</li><li>命令所在路径：/bin/grep</li><li>执行权限：所有用户</li><li>功能描述：在文件中搜寻字符串匹配的行并输出</li><li>语法：grep -iv 【指定字符串】【文件】<br>　　　　　　 -i 不区分大小写<br>　　　　　　 -v 排除指定字符串</li></ol><p>范例：查找 /root/install.log 文件中包含 mysql 字符串的行，并输出</p><blockquote><p>grep  mysql  /root/install.log</p></blockquote>]]></content>
    
    <summary type="html">
    
      find/locate/which/whereis/grep
    
    </summary>
    
      <category term="linux" scheme="http://blog.linzhongtai.cn/categories/linux/"/>
    
    
      <category term="linux" scheme="http://blog.linzhongtai.cn/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>网络IO模型</title>
    <link href="http://blog.linzhongtai.cn/2019/06/%E7%BD%91%E7%BB%9CIO%E6%A8%A1%E5%9E%8B/"/>
    <id>http://blog.linzhongtai.cn/2019/06/网络IO模型/</id>
    <published>2019-06-04T04:20:58.000Z</published>
    <updated>2019-06-04T04:20:58.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h2 id="网络IO的模型大致包括下面几种"><a href="#网络IO的模型大致包括下面几种" class="headerlink" title="网络IO的模型大致包括下面几种"></a>网络IO的模型大致包括下面几种</h2><ul><li>同步模型（synchronous IO）<ul><li>阻塞IO（bloking IO）</li><li>非阻塞IO（non-blocking IO）</li><li>多路复用IO（multiplexing IO）</li><li>信号驱动式IO（signal-driven IO）</li></ul></li><li>异步IO（asynchronous IO）<ul><li>异步IO</li></ul></li></ul><p>网络IO的本质是socket的读取，socket在linux系统被抽象为流，IO可以理解为对流的操作。对于一次IO访问，数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间，所以一般会经历两个阶段：</p><ol><li>等待所有数据都准备好或者一直在等待数据，有数据的时候将数据拷贝到系统内核；</li><li>将内核缓存中数据拷贝到用户进程中；</li></ol><p>对于socket流而言：</p><ol><li>等待网络上的数据分组到达，然后被复制到内核的某个缓冲区；</li><li>把数据从内核缓冲区复制到应用进程缓冲区中；</li></ol><h2 id="同步阻塞IO"><a href="#同步阻塞IO" class="headerlink" title="同步阻塞IO"></a>同步阻塞IO</h2><blockquote><p>在JDK 1.4之前，主要就是同步阻塞IO模型，在Java里叫做<strong>BIO</strong>。</p></blockquote><p>在Java代码里调用IO相关接口，发起IO操作之后，Java程序就会同步等待，这个同步指的是Java程序调用IO API接口的层面而言。<br>而IO API在底层的IO操作是基于阻塞IO来的，向操作系统内核发起IO请求，系统内核会等待数据就位之后，才会执行IO操作，执行完毕了才会返回。</p><h2 id="同步非阻塞NIO"><a href="#同步非阻塞NIO" class="headerlink" title="同步非阻塞NIO"></a>同步非阻塞NIO</h2><blockquote><p>在JDK 1.4之后提供了NIO，概念是同步非阻塞。</p></blockquote><p>如果你调用NIO接口去执行IO操作，其实还是同步等待的，但是在底层的IO操作上 ，会对系统内核发起非阻塞IO请求，以非阻塞的形式来执行IO。<br>也就是说，如果底层数据没到位，那么内核返回异常信息，不会阻塞住，但是NIO接口内部会采用非阻塞方式过一会儿再次调用内核发起IO请求，直到成功为止。<br>但是之所以说是同步非阻塞，这里的“同步”指的就是因为在你的Java代码调用NIO接口层面是同步的，你还是要同步等待底层IO操作真正完成了才可以返回，只不过在执行底层IO的时候采用了非阻塞的方式来执行罢了。</p><h2 id="IO多路复用"><a href="#IO多路复用" class="headerlink" title="IO多路复用"></a>IO多路复用</h2><p>之前的同步非阻塞方式需要用户进程不停的轮询，但是IO多路复用不需要不停的轮询，而是派别人去帮忙循环查询多个任务的完成状态，UNIX/Linux 下的 <strong>select、poll、epoll</strong> 就是干这个的；select调用是内核级别的，select轮询相对非阻塞的轮询的区别在于—前者可以等待多个socket，能实现同时对多个IO端口进行监听，当其中任何一个socket的数据准好了，就能返回进行可读，然后进程再进行recvform系统调用，将数据由内核拷贝到用户进程，当然这个过程是阻塞的。select或poll调用之后，会阻塞进程，与blocking IO阻塞不同在于，此时的select不是等到socket数据全部到达再处理, 而是有了一部分数据（网络上的数据是分组到达的）就会调用用户进程来处理。监视的事情交给了内核，内核负责数据到达的处理。</p><p>以上总结就是：</p><ol><li>对多个socket进行监听，只要任何一个socket数据准备好就返回可读；</li><li>不等一个socket数据全部到达再处理，而是一部分socket的数据到达了就通知用户进程；</li></ol><blockquote><p>其实 select、poll、epoll 的原理就是不断的遍历所负责的所有的socket完成状态，当某个socket有数据到达了，就返回可读并通知用户进程来处理；</p></blockquote><h2 id="AIO以及异步IO"><a href="#AIO以及异步IO" class="headerlink" title="AIO以及异步IO"></a>AIO以及异步IO</h2><blockquote><p>JDK 1.7之后，又支持了AIO，也叫做NIO 2.0，他就支持异步IO模型。</p></blockquote><p>我们先说一下异步IO模型是什么意思。<br>简单来说，就是你的Java程序可以基于AIO API发起一个请求，比如说接收网络数据，AIO API底层会基于异步IO模型来调用操作系统内核。<br>此时不需要去管这个IO是否成功了，AIO接口会直接返回，你的Java程序也会直接返回。<br>因为BIO、NIO都是同步的，你发起IO请求，都必须同步等待IO操作完成。但是这里你发起一个IO请求，直接AIO接口就返回了，你就可以干别的事儿了，纯异步的方式。<br>不过你需要提供一个回调函数给AIO接口，一旦底层系统内核完成了具体的IO请求，比如网络读写之类的，就会回调你提供的回调函数。比如说你要是通过网络读取数据，那么此时AIO接口就会把操作系统异步读取到的数据交给你的回调函数。<br>整个过程如下图：<br><img src="https://upload-images.jianshu.io/upload_images/5937589-f7c8114bf6407354.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="微信图片_20190604123852.jpg"></p>]]></content>
    
    <summary type="html">
    
      同步、异步、阻塞、非阻塞
    
    </summary>
    
      <category term="IO" scheme="http://blog.linzhongtai.cn/categories/IO/"/>
    
    
      <category term="IO" scheme="http://blog.linzhongtai.cn/tags/IO/"/>
    
  </entry>
  
  <entry>
    <title>Spring Boot 属性加载顺序</title>
    <link href="http://blog.linzhongtai.cn/2019/05/Spring-Boot-%E5%B1%9E%E6%80%A7%E5%8A%A0%E8%BD%BD%E9%A1%BA%E5%BA%8F/"/>
    <id>http://blog.linzhongtai.cn/2019/05/Spring-Boot-属性加载顺序/</id>
    <published>2019-05-28T03:48:24.000Z</published>
    <updated>2019-05-28T03:48:24.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><ol><li>在命令行中传入的参数</li><li>SPRING_APPLICATION_JSON 中的属性，SPRING_APPLICATION_JSON 是以JSON格式配置在系统环境变量中的内容</li><li>java:comp/env 中的JNI属性</li><li>Java的系统属性，可以通过System.getProperties()获得的内容</li><li>操作系统的环境变量</li><li>通过random.* 配置的随机属性</li><li><strong>位于当前应用 jar 包之外</strong>，针对不同{profile}环境的配置文件内容；例如application-{profile}.properties或是YAML定义的配置文件</li><li><strong>位于当前应用 jar 包之内</strong>，针对不同{profile}环境的配置文件内容；例如application-{profile}.properties或是YAML定义的配置文件</li><li><strong>位于当前应用 jar 包之外</strong>的application.properties和YAML配置内容</li><li><strong>位于当前应用 jar 包之内</strong>的application.properties和YAML配置内容</li><li>在@Configuration注解修改的类中，通过@PropertiesSource注解定义的属性</li><li>应用默认属性，使用SpringApplication.setDefaultProperties定义的内容</li></ol><blockquote><p>优先级由高到低，数字越小，优先级越高</p></blockquote><p>第7，9都是用应用jar包之外来读取配置文件，因此可以从此处进行切入，指定外部配置文件的加载位置来取代jar包之内的配置内容。</p>]]></content>
    
    <summary type="html">
    
      属性加载顺序
    
    </summary>
    
      <category term="SpringBoot" scheme="http://blog.linzhongtai.cn/categories/SpringBoot/"/>
    
    
      <category term="SpringBoot" scheme="http://blog.linzhongtai.cn/tags/SpringBoot/"/>
    
  </entry>
  
  <entry>
    <title>SpringBoot常用配置</title>
    <link href="http://blog.linzhongtai.cn/2019/05/SpringBoot%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/"/>
    <id>http://blog.linzhongtai.cn/2019/05/SpringBoot常用配置/</id>
    <published>2019-05-28T01:42:17.000Z</published>
    <updated>2019-05-28T01:42:17.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h2 id="Bean的Scope"><a href="#Bean的Scope" class="headerlink" title="Bean的Scope"></a>Bean的Scope</h2><ul><li>Singleton：一个Spring容器中只有一个Bean的实例，此为Spring的默认配置，全容器共享一个实例</li><li>Prototype：每次调用新建一个Bean的实例</li><li>Request：Web项目中，给每一个http request新建一个Bean实例</li><li>Session：Web项目中，给每一个http session新建一个Bean实例</li><li>GlobalSession：只在portal应用中有用，给每一个global http session新建一个Bean实例</li></ul><h2 id="Spring-EL-和资源调节"><a href="#Spring-EL-和资源调节" class="headerlink" title="Spring EL 和资源调节"></a>Spring EL 和资源调节</h2><p>Spring主要在注解@Value的参数中使用表达式：</p><ol><li>注入普通字符串</li><li>注入操作系统属性</li><li>注入表达式运算结果</li><li>注入其他Bean的属性</li><li>注入文件内容</li><li>注入网址内容</li><li>注入属性文件</li></ol><h2 id="Bean的初始化和销毁"><a href="#Bean的初始化和销毁" class="headerlink" title="Bean的初始化和销毁"></a>Bean的初始化和销毁</h2><blockquote><p>在Bean使用之前或者之后要做些必要的操作，Spring对Bean的生命周期的操作提供了支持</p></blockquote><ol><li>Java配置方式：使用@Bean的initMethod和destoryMethod（相当于xml配置的init-method和destory-method）</li><li>注解方式：利用JSR-250的@PostConstruct和@PreDestory</li></ol><h2 id="多线程"><a href="#多线程" class="headerlink" title="多线程"></a>多线程</h2><p>使用ThreadPoolTaskExecutor可实现一个基于线程池的TaskExecutor</p><blockquote><p> 在配置类中通过<strong>@EnableAsync</strong>开启对异步任务的支持，并通过在执行方法中使用<strong>@Async</strong>注解赖声明为异步任务</p></blockquote><h2 id="计划任务"><a href="#计划任务" class="headerlink" title="计划任务"></a>计划任务</h2><blockquote><p>首先在配置类注解<strong>@EnableScheduling</strong>来开启对计划任务的支持，然后在执行计划任务的方法上注解<strong>@Scheduled</strong>，声明为计划任务</p></blockquote><p>@Scheduled支持多种类型的计划任务，包含cron、fixDelay、fixRate等等</p>]]></content>
    
    <summary type="html">
    
      SpringBoot常用配置
    
    </summary>
    
      <category term="SpringBoot" scheme="http://blog.linzhongtai.cn/categories/SpringBoot/"/>
    
    
      <category term="SpringBoot" scheme="http://blog.linzhongtai.cn/tags/SpringBoot/"/>
    
  </entry>
  
  <entry>
    <title>CyclicBarrier、CountDownLatch、Semaphore的用法</title>
    <link href="http://blog.linzhongtai.cn/2019/04/CyclicBarrier%E3%80%81CountDownLatch%E3%80%81Semaphore%E7%9A%84%E7%94%A8%E6%B3%95/"/>
    <id>http://blog.linzhongtai.cn/2019/04/CyclicBarrier、CountDownLatch、Semaphore的用法/</id>
    <published>2019-04-29T09:49:05.000Z</published>
    <updated>2019-04-29T09:49:05.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h2 id="CyclicBarrier回环栅栏"><a href="#CyclicBarrier回环栅栏" class="headerlink" title="CyclicBarrier回环栅栏"></a>CyclicBarrier回环栅栏</h2><blockquote><p>等待至 barrier 状态再全部同时执行<br><strong>可重用</strong></p></blockquote><p>通过它可以实现让一组线程等待至某个状态之后再全部同时执行。叫做回环是因为当所有等待线程都被释放以后，CyclicBarrier 可以被重用。</p><p>CyclicBarrier 中最重要的方法就是 await 方法，它有 2 个重载版本：</p><ol><li>public int await()：用来挂起当前线程，直至所有线程都到达 barrier 状态再同时执行后续任务；</li><li>public int await(long timeout, TimeUnit unit)：让这些线程等待至一定的时间，如果还有线程没有到达 barrier 状态就直接让到达 barrier 的线程执行后续任务</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">public static void main(String[] args) &#123;</span><br><span class="line">  int N = 4;</span><br><span class="line">  CyclicBarrier barrier = new CyclicBarrier(N);</span><br><span class="line">  for(int i=0;i&lt;N;i++)</span><br><span class="line">    new Writer(barrier).start();</span><br><span class="line">  &#125;</span><br><span class="line">  static class Writer extends Thread&#123;</span><br><span class="line">    private CyclicBarrier cyclicBarrier;</span><br><span class="line">    public Writer(CyclicBarrier cyclicBarrier) &#123;</span><br><span class="line">      this.cyclicBarrier = cyclicBarrier;</span><br><span class="line">    &#125;</span><br><span class="line">    @Override</span><br><span class="line">    public void run() &#123;</span><br><span class="line">      try &#123;</span><br><span class="line">        Thread.sleep(5000); //以睡眠来模拟线程需要预定写入数据操作</span><br><span class="line">        System.out.println(&quot;线程&quot;+Thread.currentThread().getName()+&quot;写入数据完毕，等待其他线程写入完毕&quot;);</span><br><span class="line">        cyclicBarrier.await();</span><br><span class="line">      &#125; catch (InterruptedException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">      &#125;catch(BrokenBarrierException e)&#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">      &#125;</span><br><span class="line">      System.out.println(&quot;所有线程写入完毕，继续处理其他任务，比如数据操作&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>CyclicBarrier(int parties)：参数表示屏障拦截的线程数，每个线程调用await方法告CyclicBarrier已到达屏障，然后阻塞当前线程。</p><h2 id="CounDownLatch线程计数器"><a href="#CounDownLatch线程计数器" class="headerlink" title="CounDownLatch线程计数器"></a>CounDownLatch线程计数器</h2><blockquote><p>任务A，需要等其他四个任务完毕后，才执行。<br><strong>不可重用</strong></p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">final CountDownLatch latch = new CountDownLatch(2);</span><br><span class="line">  new Thread()&#123;public void run() &#123;</span><br><span class="line">      System.out.println(&quot;子线程&quot;+Thread.currentThread().getName()+&quot;正在执行&quot;);</span><br><span class="line">      Thread.sleep(3000);</span><br><span class="line">      System.out.println(&quot;子线程&quot;+Thread.currentThread().getName()+&quot;执行完毕&quot;);</span><br><span class="line">      latch.countDown();</span><br><span class="line">  &#125;;</span><br><span class="line">&#125;.start();</span><br><span class="line">  new Thread()&#123; public void run() &#123;</span><br><span class="line">      System.out.println(&quot;子线程&quot;+Thread.currentThread().getName()+&quot;正在执行&quot;);</span><br><span class="line">       Thread.sleep(3000);</span><br><span class="line">      System.out.println(&quot;子线程&quot;+Thread.currentThread().getName()+&quot;执行完毕&quot;);</span><br><span class="line">      latch.countDown();</span><br><span class="line">  &#125;;</span><br><span class="line">&#125;.start();</span><br><span class="line">System.out.println(&quot;等待 2 个子线程执行完毕...&quot;);</span><br><span class="line">latch.await();</span><br><span class="line">System.out.println(&quot;2 个子线程已经执行完毕&quot;);</span><br><span class="line">System.out.println(&quot;继续执行主线程&quot;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>等待N个线程，调用countDown，N减1，CountDownLatch的await方法会阻塞当前线程，直到N变成0，countDown可以放在线程中，也可以放到执行步骤中。</p><h2 id="SemaPhore信号量"><a href="#SemaPhore信号量" class="headerlink" title="SemaPhore信号量"></a>SemaPhore信号量</h2><blockquote><p>控制同时访问的线程个数.<br>通过acquire() 获取一个许可，如果没有就等待，而 release() 释放一个许可。</p></blockquote><p>Semaphore 类中比较重要的几个方法：</p><ol><li>public void acquire(): 用来获取一个许可，若无许可能够获得，则会一直等待，直到获得许可。</li><li>public void acquire(int permits):获取 permits 个许可</li><li>public void release() { } :释放许可。注意，在释放许可之前，必须先获获得许可。</li><li>public void release(int permits) { }:释放 permits 个许可</li></ol><p>上面 4 个方法都会被阻塞，如果想立即得到执行结果，可以使用下面几个方法</p><ol><li>public boolean tryAcquire():尝试获取一个许可，若获取成功，则立即返回 true，若获取失败，则立即返回 false</li><li>public boolean tryAcquire(long timeout, TimeUnit unit):尝试获取一个许可，若在指定的时间内获取成功，则立即返回 true，否则则立即返回 false</li><li>public boolean tryAcquire(int permits):尝试获取 permits 个许可，若获取成功，则立即返回 true，若获取失败，则立即返回 false</li><li>public boolean tryAcquire(int permits, long timeout, TimeUnit unit): 尝试获取 permits个许可，若在指定的时间内获取成功，则立即返回 true，否则则立即返回 false</li><li>还可以通过 availablePermits()方法得到可用的许可数目。</li></ol><p>例子：若一个工厂有5 台机器，但是有8个工人，一台机器同时只能被一个工人使用，只有使用完了，其他工人才能继续使用。那么我们就可以通过 Semaphore 来实现：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">int N = 8; //工人数</span><br><span class="line">Semaphore semaphore = new Semaphore(5); //机器数目</span><br><span class="line">for(int i=0;i&lt;N;i++)&#123;</span><br><span class="line">  new Worker(i,semaphore).start();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">static class Worker extends Thread&#123;</span><br><span class="line"></span><br><span class="line">private int num;</span><br><span class="line">private Semaphore semaphore;</span><br><span class="line">public Worker(int num,Semaphore semaphore)&#123;</span><br><span class="line">  this.num = num;</span><br><span class="line">  this.semaphore = semaphore;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@Override</span><br><span class="line">public void run() &#123;</span><br><span class="line">  try &#123;</span><br><span class="line">    semaphore.acquire();</span><br><span class="line">    System.out.println(&quot;工人&quot;+this.num+&quot;占用一个机器在生产...&quot;);</span><br><span class="line">    Thread.sleep(2000);</span><br><span class="line">    System.out.println(&quot;工人&quot;+this.num+&quot;释放出机器&quot;);</span><br><span class="line">    semaphore.release();</span><br><span class="line">  &#125; catch (InterruptedException e) &#123;</span><br><span class="line">    e.printStackTrace();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      回环栅栏、线程计数器、信号量
    
    </summary>
    
      <category term="JAVA" scheme="http://blog.linzhongtai.cn/categories/JAVA/"/>
    
    
      <category term="JAVA" scheme="http://blog.linzhongtai.cn/tags/JAVA/"/>
    
      <category term="并发编程" scheme="http://blog.linzhongtai.cn/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>AQS与CAS</title>
    <link href="http://blog.linzhongtai.cn/2019/04/AQS%E4%B8%8ECAS/"/>
    <id>http://blog.linzhongtai.cn/2019/04/AQS与CAS/</id>
    <published>2019-04-29T09:29:16.000Z</published>
    <updated>2019-04-29T09:29:16.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h2 id="AQS（AbstractQueueSynchronizer）抽象队列同步器"><a href="#AQS（AbstractQueueSynchronizer）抽象队列同步器" class="headerlink" title="AQS（AbstractQueueSynchronizer）抽象队列同步器"></a>AQS（AbstractQueueSynchronizer）抽象队列同步器</h2><blockquote><p>维护了一个 int state（代表共享资源，volatile修饰）和一个 FIFO 线程等待队列（多线程争用资源被<br>阻塞时会进入此队列）</p></blockquote><h3 id="资源共享方式"><a href="#资源共享方式" class="headerlink" title="资源共享方式"></a>资源共享方式</h3><ul><li>独占—&gt;ReentrantLock</li><li>共享—&gt;Semaphore/CountDownLatch</li><li>独占+共享—&gt;ReentrantReadWriteLock</li></ul><h2 id="CAS（Compare-And-Swap-Set）比较并交换-乐观锁机制-锁自旋"><a href="#CAS（Compare-And-Swap-Set）比较并交换-乐观锁机制-锁自旋" class="headerlink" title="CAS（Compare And Swap/Set）比较并交换-乐观锁机制-锁自旋"></a>CAS（Compare And Swap/Set）比较并交换-乐观锁机制-锁自旋</h2><blockquote><p>当且仅当内存值V等于预期值A，才会将内存值设置为新值N</p></blockquote><ul><li>锁自旋—&gt;AtomicInteger.getAndIncrement</li><li>ABA问题—&gt;版本号解决</li></ul>]]></content>
    
    <summary type="html">
    
      什么是AQS？什么是CAS？
    
    </summary>
    
      <category term="JAVA" scheme="http://blog.linzhongtai.cn/categories/JAVA/"/>
    
    
      <category term="JAVA" scheme="http://blog.linzhongtai.cn/tags/JAVA/"/>
    
      <category term="小知识" scheme="http://blog.linzhongtai.cn/tags/%E5%B0%8F%E7%9F%A5%E8%AF%86/"/>
    
  </entry>
  
  <entry>
    <title>Redis缓存淘汰策略</title>
    <link href="http://blog.linzhongtai.cn/2019/04/Redis%E7%BC%93%E5%AD%98%E6%B7%98%E6%B1%B0%E7%AD%96%E7%95%A5/"/>
    <id>http://blog.linzhongtai.cn/2019/04/Redis缓存淘汰策略/</id>
    <published>2019-04-29T09:13:17.000Z</published>
    <updated>2019-04-29T09:13:17.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h2 id="常用的淘汰算法"><a href="#常用的淘汰算法" class="headerlink" title="常用的淘汰算法"></a>常用的淘汰算法</h2><ul><li>FIFO：First In First Out，先进先出。判断被存储的时间，离目前最远的数据优先被淘汰。</li><li>LRU：Least Recently Used，最近最少使用。判断最近被使用的时间，目前最远的数据优先被淘汰。</li><li>LFU：Least Frequently Used，最不经常使用。在一段时间内，数据被使用次数最少的，优先被淘汰。</li></ul><h2 id="Redis提供的淘汰策略："><a href="#Redis提供的淘汰策略：" class="headerlink" title="Redis提供的淘汰策略："></a>Redis提供的淘汰策略：</h2><ul><li>noeviction：达到内存限额后返回错误，客户尝试可以导致更多内存使用的命令（大部分写命令，但DEL和一些例外）</li><li>allkeys-lru：为了给新增加的数据腾出空间，驱逐键先试图移除一部分最近使用较少的（LRC）。</li><li>volatile-lru：为了给新增加的数据腾出空间，驱逐键先试图移除一部分最近使用较少的（LRC），但只限于过期设置键。</li><li>allkeys-random: 为了给新增加的数据腾出空间，驱逐任意键</li><li>volatile-random: 为了给新增加的数据腾出空间，驱逐任意键，但只限于有过期设置的驱逐键。</li><li>volatile-ttl: 为了给新增加的数据腾出空间，驱逐键只有秘钥过期设置，并且首先尝试缩短存活时间的驱逐键</li></ul>]]></content>
    
    <summary type="html">
    
      LRU
    
    </summary>
    
      <category term="redis" scheme="http://blog.linzhongtai.cn/categories/redis/"/>
    
    
      <category term="redis" scheme="http://blog.linzhongtai.cn/tags/redis/"/>
    
  </entry>
  
  <entry>
    <title>Redis中keys与scan特点</title>
    <link href="http://blog.linzhongtai.cn/2019/04/Redis%E4%B8%ADkeys%E4%B8%8Escan%E7%89%B9%E7%82%B9/"/>
    <id>http://blog.linzhongtai.cn/2019/04/Redis中keys与scan特点/</id>
    <published>2019-04-29T09:06:41.000Z</published>
    <updated>2019-04-29T09:06:41.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><h2 id="keys缺点"><a href="#keys缺点" class="headerlink" title="keys缺点"></a>keys缺点</h2><ol><li>没有offset、limit参数，不能限制查询个数</li><li>keys是遍历算法，复杂度O(n)，数据量大的时候会导致redis卡顿</li></ol><h2 id="scan"><a href="#scan" class="headerlink" title="scan"></a>scan</h2><ol><li>复杂度O(n)，但是scan是通过游标分步进行，不阻塞</li><li>提供limit，可控制返回结果数</li><li>同keys一样，提供模式匹配</li><li>服务器不需要为游标保存状态，唯一状态是scan返回客户端的游标整数</li><li><strong>返回结果可能重复，需要客户端去重</strong></li><li>如果遍历过程中有数据修改，改动后的数据不保证同步</li><li>单次返回结果是空的，不表示遍历结束，而要看返回的游标值是否为0</li></ol>]]></content>
    
    <summary type="html">
    
      keys VS scan
    
    </summary>
    
      <category term="redis" scheme="http://blog.linzhongtai.cn/categories/redis/"/>
    
    
      <category term="redis" scheme="http://blog.linzhongtai.cn/tags/redis/"/>
    
  </entry>
  
</feed>
